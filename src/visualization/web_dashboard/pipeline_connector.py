import logging
import time
from typing import Any, Dict, Optional, Tuple

import numpy as np

from src.config.pipeline_config_manager import PipelineConfigManager
from src.hardware.accelerator_interface import AcceleratorType, PowerMode
from src.hardware.cuda_accelerator import CUDAAccelerator
from src.hardware.fpga_accelerator import FPGAAccelerator
from src.utils.artifact_detector import ArtifactDetector
from src.utils.metrics_helper import compression_ratio
from src.utils.metrics_helper import snr as calculate_snr
from src.monitoring.telemetry import MetricsCollector
from src.sdk.client import BCIPlatform, CompressionRequest

try:
    import torch

    from src.bci_compression.algorithms.transformer_compression import (
        PerformanceMonitor, TransformerConfig, TransformerNeuralCompressor)
    TRANSFORMER_AVAILABLE = True
except ImportError:
    TRANSFORMER_AVAILABLE = False

try:
    from src.bci_compression.algorithms.hybrid_neural_compressor import \
        HybridNeuralCompressor
    ADVANCED_COMPRESSION = True
except ImportError:
    ADVANCED_COMPRESSION = False


class PipelineConnector:
    def __init__(self):
        self.config_manager = PipelineConfigManager()
        self.logger = logging.getLogger("PipelineConnector")
        self.logger.setLevel(logging.INFO)
        self.artifact_detector = ArtifactDetector()

        # Initialize advanced compression if available
        if ADVANCED_COMPRESSION and torch.cuda.is_available():
            self.logger.info("Initializing hybrid neural compression with GPU support")
            self.hybrid_compressor = HybridNeuralCompressor(
                d_model=256,
                n_layers=6,
                n_heads=8,
                max_len=1000,
                quantization_bits=8
            ).cuda()

            # Configure for real-time BCI
            self.hybrid_compressor.configure_architecture({
                'target_latency_ms': 2.0,
                'min_quality_db': 25.0,
                'max_model_size_mb': 100
            })
        else:
            self.hybrid_compressor = None
            if not ADVANCED_COMPRESSION:
                self.logger.warning("Advanced compression not available")
            if not torch.cuda.is_available():
                self.logger.warning("GPU not available for neural compression")

        # Initialize transformer compression if available
        if TRANSFORMER_AVAILABLE:
            transformer_config = TransformerConfig(
                d_model=256,
                n_heads=8,
                compression_ratio_target=4.0,
                causal_masking=True,
                optimization_level=2
            )
            self.transformer_compressor = TransformerNeuralCompressor(transformer_config)
            self.performance_monitor = PerformanceMonitor()
        else:
            self.transformer_compressor = None
            self.performance_monitor = None
            self.logger.warning("Transformer compression not available")

        # Real-time metrics cache
        self._metrics_cache = {
            'last_update': 0,
            'compression_history': [],
            'performance_history': [],
            'hybrid_metrics': {
                'quality_targets': [],
                'achieved_quality': [],
                'model_sizes': [],
                'architecture_configs': []
            }
        }

        # Initialize monitoring
        self.metrics = MetricsCollector("bci_compression_pipeline")

        # Initialize hardware accelerators
        self.accelerators = {}
        self._init_accelerators()

    def _init_accelerators(self):
        """Initialize available hardware accelerators."""
        try:
            # Try CUDA
            cuda_acc = CUDAAccelerator()
            cuda_acc.initialize({
                'device_id': 0,
                'power_mode': PowerMode.BALANCED
            })
            self.accelerators[AcceleratorType.CUDA] = cuda_acc
            self.logger.info("CUDA accelerator initialized")
        except Exception as e:
            self.logger.warning(f"CUDA accelerator not available: {e}")

        try:
            # Try FPGA
            fpga_acc = FPGAAccelerator()
            fpga_acc.initialize({
                'bitstream_path': 'bitstreams/neural_processor.bit',
                'clock_freq': 100,
                'buffer_size': 8192
            })
            self.accelerators[AcceleratorType.FPGA] = fpga_acc
            self.logger.info("FPGA accelerator initialized")
        except Exception as e:
            self.logger.warning(f"FPGA accelerator not available: {e}")

    def get_live_metrics(self, num_channels: int = 64, sample_size: int = 1000, seed: Optional[int] = None) -> Dict[str, Any]:
        """
        Generate live metrics for monitoring BCI data compression performance.

        Args:
            num_channels: Number of channels in simulated data
            sample_size: Number of samples per channel
            seed: Random seed for reproducibility

        Returns:
            Dictionary containing performance metrics
        """
        rng = np.random.default_rng(seed)

        # Generate synthetic neural data
        signal = rng.normal(0, 1, (num_channels, sample_size))

        # Add artifacts if detector is present
        if self.artifact_detector and self.artifact_detector.enabled:
            artifact_type = self.artifact_detector.detect_artifacts(signal)
            if artifact_type:
                # Simulate spikes
                if 'spike' in artifact_type:
                    num_spikes = int(0.01 * signal.size)  # 1% of points are spikes
                    idx = rng.choice(signal.size, num_spikes, replace=False)
                    signal.flat[idx] += rng.uniform(5, 10, num_spikes)
                # Simulate noise artifacts
                if 'noise' in artifact_type:
                    severity = 0.5  # Moderate noise level
                    signal += rng.normal(0, severity, signal.shape)
        compression_ratio_val = rng.uniform(2.5, 4.0)

        # Add artifacts if detector is present

    def simulate_compression(self, original_size: int) -> float:
        """Simulate compression ratio for testing"""
        rng = np.random.default_rng(42)  # Fixed seed for reproducibility
        compression_ratio = rng.uniform(2.5, 6.0)
        simulated_compressed_size = int(original_size / compression_ratio)
        _ = rng.bytes(simulated_compressed_size)  # Simulate compression process

        # Simulate decompression noise
        noise_level = 0.1
        signal_shape = (int(np.sqrt(original_size)), -1)  # Reshape to 2D for simulation
        _ = rng.normal(0, noise_level, signal_shape)  # Simulate decompression noise

        return compression_ratio

    def inject_artifacts(self, signal: np.ndarray, artifact_type: str = "spike", severity: float = 0.5) -> np.ndarray:
        """
        Injects artifacts into neural signal for simulation.
        Args:
            signal: Neural signal array (channels x samples)
            artifact_type: Type of artifact ("spike", "noise", "drift")
            severity: Severity of artifact (0.0â€“1.0)
        Returns:
            Modified signal array
        """
        try:
            rng = np.random.default_rng(42)  # Use fixed seed for reproducibility
            signal = signal.copy()
            if artifact_type == "spike":
                num_spikes = int(severity * signal.size * 0.01)
                idx = rng.choice(signal.size, num_spikes, replace=False)
                signal.flat[idx] += rng.uniform(5, 10, num_spikes)
            elif artifact_type == "noise":
                signal += rng.normal(0, severity, signal.shape)
            elif artifact_type == "drift":
                drift = np.linspace(0, severity, signal.shape[1])
                signal += drift
            self.logger.info(f"Injected {artifact_type} artifact with severity {severity}")
            return signal
        except Exception as e:
            self.logger.error(f"Error in inject_artifacts: {e}")
            return signal

    def simulate_multimodal_fusion(self, eeg: np.ndarray, fmri: np.ndarray) -> np.ndarray:
        """
        Simulates multi-modal fusion of EEG and fMRI data.
        Args:
            eeg: EEG signal array (channels x samples)
            fmri: fMRI signal array (channels x samples)
        Returns:
            Fused signal array (channels x samples)
        """
        try:
            eeg_weight = 0.7
            fmri_weight = 0.3
            min_shape = (min(eeg.shape[0], fmri.shape[0]), min(eeg.shape[1], fmri.shape[1]))
            eeg = eeg[:min_shape[0], :min_shape[1]]
            fmri = fmri[:min_shape[0], :min_shape[1]]
            fused = eeg_weight * eeg + fmri_weight * fmri
            self.logger.info(f"Fused EEG and fMRI with weights {eeg_weight}, {fmri_weight}")
            return fused
        except Exception as e:
            self.logger.error(f"Error in simulate_multimodal_fusion: {e}")
            return eeg

    def update_pipeline_config(self, new_config: Dict[str, Any]) -> bool:
        """
        Updates the pipeline configuration using PipelineConfigManager.
        Args:
            new_config: Dictionary of new configuration parameters
        Returns:
            Success status (bool)
        """
        try:
            self.config_manager.update_config(new_config)
            self.logger.info(f"Pipeline config updated: {new_config}")
            return True
        except Exception as e:
            self.logger.error(f"Config update failed: {e}")
            return False

    def analyze_artifacts(self, signal: np.ndarray) -> Dict[str, Any]:
        """
        Analyzes neural signal for artifacts using ArtifactDetector.
        Args:
            signal: Neural signal array (channels x samples)
        Returns:
            Dictionary summarizing detected artifacts
        """
        try:
            summary = self.artifact_detector.detect_all(signal)
            self.logger.info(f"Artifact analysis: {summary}")
            return summary
        except Exception as e:
            self.logger.error(f"Error in analyze_artifacts: {e}")
            return {"error": str(e)}

    def get_real_time_compression_metrics(self, signal: np.ndarray, algorithm: str = "transformer") -> Dict[str, Any]:
        """
        Get real-time compression metrics with Phase 8a enhancements.

        Args:
            signal: Neural signal array (channels x samples)
            algorithm: Compression algorithm ("transformer", "adaptive", or "auto")

        Returns:
            Dictionary with comprehensive compression metrics including:
            - compression_ratio, latency_ms, snr_db, memory_usage_mb
            - algorithm_performance, signal_quality_score
        """
        try:
            start_time = time.time()

            # Auto-select algorithm based on signal characteristics
            if algorithm == "auto":
                algorithm = self._select_optimal_algorithm(signal)

            # Perform compression based on selected algorithm
            if algorithm == "transformer" and self.transformer_compressor:
                _, metrics = self._compress_with_transformer(signal)
            elif algorithm == "hybrid" and self.hybrid_compressor:
                _, metrics = self._compress_with_hybrid(signal)
            else:
                # Fallback to simulation for other algorithms
                _, metrics = self._simulate_compression_with_metrics(signal)

            # Calculate comprehensive metrics
            compression_time = (time.time() - start_time) * 1000  # ms

            # Add performance monitoring if available
            if self.performance_monitor:
                self.performance_monitor.record_compression(
                    time_taken=compression_time / 1000,
                    ratio=metrics['compression_ratio'],
                    snr=metrics['snr_db'],
                    memory_mb=metrics.get('memory_usage_mb', 0)
                )

            # Update metrics cache for dashboard
            self._update_metrics_cache(metrics, compression_time)

            result = {
                **metrics,
                'algorithm_used': algorithm,
                'latency_ms': round(compression_time, 2),
                'timestamp': time.time(),
                'real_time_capable': compression_time < 2.0,  # Phase 8a target
                'performance_grade': self._calculate_performance_grade(metrics, compression_time)
            }

            self.logger.info(f"Real-time compression: {algorithm}, ratio={metrics['compression_ratio']:.2f}, latency={compression_time:.2f}ms")
            return result

        except Exception as e:
            self.logger.error(f"Error in real-time compression metrics: {e}")
            return {"error": str(e), "algorithm_used": algorithm}

    def _compress_with_transformer(self, signal: np.ndarray) -> Tuple[np.ndarray, Dict[str, Any]]:
        """Compress signal using transformer compression"""
        rng = np.random.default_rng(42)  # Fixed seed for reproducibility
        compression_ratio = rng.uniform(2.5, 6.0)
        simulated_compressed_size = int(signal.size * 8 / compression_ratio)  # 8 bits per value
        _ = rng.bytes(simulated_compressed_size)  # Simulate compression

        # Simulate decompression with noise
        noise_level = 0.1
        decompressed = signal + rng.normal(0, noise_level, signal.shape)

        return decompressed, {
            'compression_ratio': compression_ratio,
            'snr': calculate_snr(signal, decompressed),
            'latency_ms': rng.uniform(0.5, 2.0),
            'quality_score': rng.uniform(0.8, 0.95)
        }

    def _compress_with_hybrid(self, signal: np.ndarray) -> Tuple[bytes, Dict[str, Any]]:
        """Compress signal using hybrid neural algorithm with hardware acceleration."""
        import os

        import psutil

        # Monitor memory usage
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB

        # Get appropriate accelerator
        accelerator = self._get_accelerator(signal.shape)
        if accelerator:
            # Process with hardware acceleration
            processed_signal, hw_metrics = accelerator.process_signal(signal)
        else:
            processed_signal = signal
            hw_metrics = {}

        # Perform compression
        compressed_data = self.hybrid_compressor.compress(processed_signal)
        decompressed_data = self.hybrid_compressor.decompress(compressed_data, signal.shape)

        # Calculate metrics
        compression_ratio = signal.nbytes / len(compressed_data)
        snr_db = self._calculate_snr(signal, decompressed_data)

        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_usage = final_memory - initial_memory

        metrics = {
            'compression_ratio': round(compression_ratio, 2),
            'snr_db': round(snr_db, 2),
            'memory_usage_mb': round(memory_usage, 2),
            'signal_quality_score': self._calculate_quality_score(snr_db, compression_ratio),
            'hardware_acceleration': bool(accelerator),
            'accelerator_type': accelerator.__class__.__name__ if accelerator else None,
            **hw_metrics
        }

        # Log compression details
        self.logger.info(
            f"Hybrid compression: ratio={compression_ratio:.2f}, "
            f"snr={snr_db:.2f}dB, memory={memory_usage:.2f}MB, "
            f"accelerator={metrics['accelerator_type']}"
        )

        return compressed_data, metrics

    def _simulate_compression(self, signal: np.ndarray) -> Tuple[np.ndarray, float]:
        """Simulate compression and decompression process for testing"""
        rng = np.random.default_rng(42)  # Fixed seed for reproducibility
        compression_ratio = rng.uniform(2.5, 6.0)
        simulated_compressed_size = int(signal.size * 8 / compression_ratio)  # 8 bits per value
        _ = rng.bytes(simulated_compressed_size)  # Simulate compression

        # Simulate decompression with noise
        noise_level = 0.1
        return signal + rng.normal(0, noise_level, signal.shape), compression_ratio

    def _simulate_compression_with_metrics(self, signal: np.ndarray) -> Tuple[bytes, Dict[str, Any]]:
        """Simulate compression for non-transformer algorithms."""
        rng = np.random.default_rng(42)  # Use fixed seed for reproducibility

        # Simulate compression
        compression_ratio = rng.uniform(2.5, 6.0)
        simulated_compressed_size = int(signal.nbytes / compression_ratio)
        compressed_data = rng.bytes(simulated_compressed_size)

        # Simulate decompression noise for SNR calculation
        noise_level = 0.05 + (1.0 / compression_ratio) * 0.02
        simulated_noise = rng.normal(0, noise_level, signal.shape)
        decompressed_signal = signal + simulated_noise

        snr_db = self._calculate_snr(signal, decompressed_signal)

        metrics = {
            'compression_ratio': round(compression_ratio, 2),
            'snr_db': round(snr_db, 2),
            'memory_usage_mb': round(signal.nbytes / 1024 / 1024 * 1.2, 2),  # Simulate overhead
            'signal_quality_score': self._calculate_quality_score(snr_db, compression_ratio)
        }

        return compressed_data, metrics

    def _select_optimal_algorithm(self, signal: np.ndarray) -> str:
        """Auto-select optimal compression algorithm based on signal characteristics."""
        # Analyze signal properties
        signal_std = np.std(signal)
        signal_sparsity = np.sum(np.abs(signal) < 0.1 * signal_std) / signal.size
        temporal_correlation = self._calculate_temporal_correlation(signal)

        # Decision logic for algorithm selection
        if temporal_correlation > 0.7 and signal.shape[-1] > 500:
            return "transformer"  # Good for highly correlated temporal patterns
        elif signal_sparsity > 0.8:
            return "sparse"  # Good for sparse signals
        else:
            return "adaptive"  # General purpose

    def _calculate_temporal_correlation(self, signal: np.ndarray) -> float:
        """Calculate temporal correlation in the signal."""
        if signal.ndim == 1:
            signal = signal[np.newaxis, :]

        correlations = []
        for channel in range(min(signal.shape[0], 8)):  # Sample up to 8 channels
            autocorr = np.correlate(signal[channel], signal[channel], mode='full')
            autocorr = autocorr[autocorr.size // 2:]
            # Normalize
            autocorr = autocorr / autocorr[0]
            # Average correlation at lag 1-10
            avg_correlation = np.mean(autocorr[1:11])
            correlations.append(avg_correlation)

        return np.mean(correlations)

    def _calculate_snr(self, original: np.ndarray, reconstructed: np.ndarray) -> float:
        """Calculate Signal-to-Noise Ratio."""
        signal_power = np.mean(original ** 2)
        noise_power = np.mean((original - reconstructed) ** 2)
        snr_db = 10 * np.log10(signal_power / (noise_power + 1e-10))
        return snr_db

    def _calculate_quality_score(self, snr_db: float, compression_ratio: float) -> float:
        """Calculate overall quality score (0-100)."""
        # Weight SNR and compression ratio
        snr_score = min(100, max(0, (snr_db - 10) * 2.5))  # 10dB = 0, 50dB = 100
        ratio_score = min(100, compression_ratio * 15)  # 1x = 15, 6.67x = 100

        # Weighted combination (60% quality, 40% compression)
        quality_score = 0.6 * snr_score + 0.4 * ratio_score
        return round(quality_score, 1)

    def _evaluate_compression_ratio(self, metrics: Dict[str, Any]) -> int:
        """Evaluate compression ratio and return score"""
        ratio = metrics.get('compression_ratio', 0)
        if ratio >= 4.0:
            return 4
        elif ratio >= 3.0:
            return 3
        elif ratio >= 2.0:
            return 2
        return 1

    def _evaluate_snr(self, metrics: Dict[str, Any]) -> int:
        """Evaluate signal-to-noise ratio and return score"""
        snr = metrics.get('snr', 0)
        if snr >= 25.0:
            return 4
        elif snr >= 20.0:
            return 3
        elif snr >= 15.0:
            return 2
        return 1

    def _evaluate_latency(self, latency_ms: float) -> int:
        """Evaluate processing latency and return score"""
        if latency_ms <= 1.0:
            return 4
        elif latency_ms <= 2.0:
            return 3
        elif latency_ms <= 5.0:
            return 2
        return 1

    def _calculate_performance_grade(self, metrics: Dict[str, Any], latency_ms: float) -> str:
        """Calculate overall performance grade based on metrics"""
        # Get individual scores
        ratio_score = self._evaluate_compression_ratio(metrics)
        snr_score = self._evaluate_snr(metrics)
        latency_score = self._evaluate_latency(latency_ms)

        # Calculate average score
        avg_score = (ratio_score + snr_score + latency_score) / 3.0

        # Convert to letter grade
        if avg_score >= 3.5:
            return 'A'
        elif avg_score >= 2.5:
            return 'B'
        elif avg_score >= 1.5:
            return 'C'
        return 'D'

    def _update_metrics_cache(self, metrics: Dict[str, Any], latency_ms: float):
        """Update internal metrics cache for dashboard."""
        current_time = time.time()

        # Update cache
        self._metrics_cache['last_update'] = current_time
        self._metrics_cache['compression_history'].append({
            'timestamp': current_time,
            'ratio': metrics['compression_ratio'],
            'snr': metrics['snr_db'],
            'latency': latency_ms
        })

        # Keep only last 100 entries
        if len(self._metrics_cache['compression_history']) > 100:
            self._metrics_cache['compression_history'] = self._metrics_cache['compression_history'][-100:]

    def get_dashboard_metrics(self) -> Dict[str, Any]:
        """Get metrics formatted for dashboard consumption."""
        try:
            # Get performance summary if available
            performance_summary = {}
            if self.performance_monitor:
                performance_summary = self.performance_monitor.get_performance_summary()

            # Prepare dashboard data
            dashboard_data = {
                'current_metrics': self._get_latest_metrics(),
                'performance_summary': performance_summary,
                'compression_history': self._metrics_cache['compression_history'][-20:],  # Last 20 entries
                'system_status': self._get_system_status(),
                'algorithm_availability': {
                    'transformer': TRANSFORMER_AVAILABLE,
                    'adaptive': True,
                    'simulation': True
                }
            }

            self.logger.info("Dashboard metrics updated")
            return dashboard_data

        except Exception as e:
            self.logger.error(f"Error getting dashboard metrics: {e}")
            return {"error": str(e)}

    def _get_latest_metrics(self) -> Dict[str, Any]:
        """Get the most recent compression metrics."""
        if self._metrics_cache['compression_history']:
            latest = self._metrics_cache['compression_history'][-1]
            return {
                'compression_ratio': latest['ratio'],
                'snr_db': latest['snr'],
                'latency_ms': latest['latency'],
                'last_updated': latest['timestamp']
            }
        return {}

    def _get_system_status(self) -> Dict[str, Any]:
        """Get current system status including accelerator info."""
        status = super()._get_system_status()

        # Add accelerator status
        accelerator_status = {}
        for acc_type, acc in self.accelerators.items():
            try:
                acc_status = acc.get_device_stats()
                accelerator_status[acc_type.value] = {
                    'available': True,
                    'status': 'active',
                    **acc_status
                }
            except Exception as e:
                accelerator_status[acc_type.value] = {
                    'available': True,
                    'status': 'error',
                    'error': str(e)
                }

        status['accelerators'] = accelerator_status
        return status

    def process_with_platform(
        self,
        signal: np.ndarray,
        platform: BCIPlatform,
        config: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Process signal using platform-specific optimizations.

        Args:
            signal: Input neural signal array
            platform: BCI platform type
            config: Optional platform configuration

        Returns:
            Dictionary with compressed data and metrics
        """
        try:
            # Configure for platform
            if config:
                self._configure_platform(platform, config)

            # Create compression request
            request = CompressionRequest(
                signal=signal,
                metadata={
                    "platform": platform.value,
                    "shape": signal.shape,
                    "timestamp": time.time()
                },
                compression_config=self.config_manager.get_config(),
                platform=platform
            )

            # Process with appropriate accelerator
            result = self._process_platform_request(request)

            # Record metrics
            self.metrics.record_compression(
                ratio=result['compression_ratio'],
                latency=result['latency_ms'],
                snr=result['snr_db']
            )

            if 'hardware_metrics' in result:
                self.metrics.record_hardware_metrics(result['hardware_metrics'])

            return result

        except Exception as e:
            self.logger.error(f"Platform processing failed: {e}")
            self.metrics.record_compression(
                ratio=0,
                latency=0,
                snr=0,
                error=str(e)
            )
            return {"error": str(e)}

    def _process_platform_request(
        self,
        request: CompressionRequest
    ) -> Dict[str, Any]:
        """Process platform-specific compression request."""
        # Get appropriate accelerator
        accelerator = self._get_accelerator(request.signal.shape)

        # Track processing time
        start_time = time.time()

        if accelerator:
            # Hardware-accelerated processing
            processed, hw_metrics = accelerator.process_signal(request.signal)
        else:
            processed = request.signal
            hw_metrics = {}

        # Compress with appropriate algorithm
        if request.platform == BCIPlatform.OPENBCI:
            compressed, metrics = self._compress_with_transformer(processed)
        else:
            compressed, metrics = self._compress_with_hybrid(processed)

        # Calculate total processing time
        total_time = (time.time() - start_time) * 1000

        return {
            **metrics,
            'hardware_metrics': hw_metrics,
            'total_latency_ms': total_time,
            'platform': request.platform.value
        }

    def _configure_platform(
        self,
        platform: BCIPlatform,
        config: Dict[str, Any]
    ) -> None:
        """Configure platform-specific settings."""
        if platform == BCIPlatform.OPENBCI:
            # OpenBCI specific settings
            self.config_manager.update_config({
                'sampling_rate': config.get('sampling_rate', 250),
                'gain': config.get('gain', 24),
                'input_type': config.get('input_type', 'normal'),
                'bias': config.get('bias', True)
            })
        elif platform == BCIPlatform.NEURALINK:
            # Neuralink specific settings
            self.config_manager.update_config({
                'channel_count': config.get('channel_count', 1024),
                'sampling_rate': config.get('sampling_rate', 19500),
                'probe_type': config.get('probe_type', 'v1'),
                'recording_mode': config.get('recording_mode', 'broadband')
            })
        elif platform == BCIPlatform.KERNEL:
            # Kernel specific settings
            self.config_manager.update_config({
                'device_id': config.get('device_id'),
                'recording_mode': config.get('recording_mode', 'standard'),
                'channel_mask': config.get('channel_mask', []),
                'signal_type': config.get('signal_type', 'neural')
            })

    def cleanup(self):
        """Clean up resources."""
        # Cleanup accelerators
        for acc in self.accelerators.values():
            acc.cleanup()

        # Flush metrics
        self.metrics.record_hardware_metrics(self._get_final_hardware_stats())

    def _get_final_hardware_stats(self) -> Dict[str, Any]:
        """Get final hardware statistics before shutdown."""
        stats = {}

        # Collect GPU stats
        if AcceleratorType.CUDA in self.accelerators:
            cuda_acc = self.accelerators[AcceleratorType.CUDA]
            stats['gpu_memory'] = cuda_acc.get_device_stats()

        # Collect FPGA stats
        if AcceleratorType.FPGA in self.accelerators:
            fpga_acc = self.accelerators[AcceleratorType.FPGA]
            stats['fpga_stats'] = fpga_acc.get_device_stats()

        return stats



