# Brain-Computer Interface Data Compression Toolkit

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg?style=flat-square&logo=python)](https://www.python.org/downloads/)
[![Docker](https://img.shields.io/badge/docker-ready-blue.svg?style=flat-square&logo=docker)](docker/)
[![GPU Acceleration](https://img.shields.io/badge/GPU-CUDA%20%7C%20ROCm-green.svg?style=flat-square&logo=nvidia)](README.md#gpu-acceleration)
[![API Server](https://img.shields.io/badge/API-FastAPI-teal.svg?style=flat-square&logo=fastapi)](http://localhost:8000/docs)
[![Compression](https://img.shields.io/badge/compression-neural--optimized-red.svg?style=flat-square)](README.md#compression-technologies)
[![BCI](https://img.shields.io/badge/BCI-real--time-purple.svg?style=flat-square)](README.md#project-purpose)
[![Tests](https://img.shields.io/badge/tests-45%20passing-success.svg?style=flat-square)](tests/)

> **üß† A state-of-the-art toolkit for neural data compression in brain-computer interfaces**
> *Enabling real-time, lossless compression of neural signals for next-generation BCIs with GPU acceleration*

---

## ‚ú® Recent Updates (November 2025)

**üéâ Multi-Device BCI Support & Real-Time Streaming**

- ‚úÖ **4 New Device Adapters**: Blackrock (Neuroport/Cerebus), Intan (RHD/RHS), HDF5 (generic)
- ‚úÖ **Streaming Compression**: <1ms latency with circular buffers and sliding windows
- ‚úÖ **Multi-Device Pipeline**: Unified compression for simultaneous recording systems
- ‚úÖ **Performance Profiling**: Built-in tools to measure adapter overhead and optimize
- ‚úÖ **45 Tests Passing**: Comprehensive test coverage for all adapters
- ‚úÖ **Real-World Examples**: 3 complete working examples with benchmarks

See [Adapters Implementation Summary](docs/adapters_implementation_summary.md) for complete details.

---

## üìñ Table of Contents

- [üéØ Project Purpose](#-project-purpose)
- [üèóÔ∏è System Architecture](#Ô∏è-system-architecture)
- [üöÄ Quick Start](#-quick-start)
- [üîß Technology Stack](#-technology-stack)
- [‚ö° GPU Acceleration](#-gpu-acceleration)
 - [ÔøΩ Multi-BCI Systems & Electrode Mapping](#-multi-bci-systems--electrode-mapping)
 - [üß™ Testing & Benchmarks Enhancements](#-testing--benchmarks-enhancements)

---

## üéØ Project Purpose

### Why This Project Exists

Brain-Computer Interfaces (BCIs) represent one of the most promising frontiers in neuroscience and human-computer interaction. However, a critical bottleneck threatens to limit their potential: **data management**. Modern BCIs generate enormous volumes of high-dimensional neural data that must be processed, transmitted, and stored in real-time with perfect fidelity. This project exists to solve that bottleneck.

### The Problem We're Solving

**Neural data is fundamentally different from traditional data:**
- üìä **Volume**: A single 256-channel neural array at 30kHz generates **30.72 million samples per second** (15 MB/s uncompressed)
- ‚ö° **Latency**: Closed-loop BCIs require **sub-millisecond response times** for natural control
- üéØ **Fidelity**: Neural features must be preserved perfectly - even tiny distortions can break decoding algorithms
- üîã **Constraints**: Implantable and mobile BCIs have severe power and bandwidth limitations

**Traditional compression algorithms fail because:**
1. They treat neural data as generic byte streams, missing temporal and spatial patterns
2. They're optimized for text/images, not oscillating multi-channel time-series data
3. They can't guarantee real-time performance with varying signal characteristics
4. They don't preserve the specific neural features needed for BCI decoding

### Why This Matters

| Without This Toolkit | With This Toolkit |
|---------------------|-------------------|
| ‚ùå Wireless BCIs limited to minutes of recording | ‚úÖ Hours of continuous wireless neural streaming |
| ‚ùå Expensive high-bandwidth transmitters required | ‚úÖ 5-10x reduction in transmission costs |
| ‚ùå Researchers forced to downsample or select channels | ‚úÖ Full-resolution multi-channel recordings |
| ‚ùå Real-time processing limited by data bottlenecks | ‚úÖ Sub-millisecond compression for closed-loop control |
| ‚ùå Neural datasets too large to share easily | ‚úÖ Shareable compressed datasets for reproducibility |

### The Challenge We're Addressing

| Challenge | Impact | Current Solutions | Our Approach |
|-----------|--------|------------------|--------------|
| **Data Volume** | 100+ channels √ó 30kHz = 3M+ samples/sec | Basic compression (20-30% reduction) | Neural-aware algorithms (60-80% reduction) |
| **Real-time Requirements** | <1ms latency for closed-loop control | Hardware buffers, simplified algorithms | GPU-accelerated processing |
| **Signal Fidelity** | Lossless preservation of neural features | Generic compression loses critical features | BCI-specific feature preservation |
| **Resource Constraints** | Mobile/embedded devices with limited power | CPU-only, high power consumption | Optimized GPU kernels, adaptive selection |
| **Accessibility** | Expensive infrastructure required | Limited to well-funded labs | Open-source, cloud-deployable solution |

### Who Benefits From This

1. **üî¨ Researchers**: Conduct longer experiments, store more data, collaborate easier
2. **üè• Medical Professionals**: Enable real-time neural monitoring, telemedicine applications
3. **üè¢ BCI Companies**: Reduce hardware costs, enable mobile/implantable devices
4. **‚ôø End Users**: Better BCI performance, more affordable assistive devices
5. **üåç Neuroscience Community**: Shared compression standard for reproducible research

### Target Applications

```mermaid
mindmap
  root((üß† BCI Data Compression))
    üéØ Applications
      ü¶æ Motor BCIs
        Prosthetic Control
        Robotic Arms
        Wheelchair Navigation
      üß† Cognitive BCIs
        Speech Synthesis
        Memory Enhancement
        Attention Monitoring
      üè• Medical BCIs
        Epilepsy Monitoring
        Depression Treatment
        Sleep Analysis
      üì± Consumer BCIs
        Gaming Interfaces
        VR/AR Control
        Meditation Apps
    üìä Data Types
      üîå Neural Signals
        Spike Trains
        Local Field Potentials
        ECoG Arrays
      üìà Biosignals
        EMG Patterns
        EEG Recordings
        fMRI Data
    ‚ö° Performance Goals
      üöÄ Speed
        <1ms Latency
        Real-time Processing
        Streaming Compatible
      üíæ Efficiency
        60-80% Compression
        Lossless Quality
        Adaptive Selection
```

### Key Innovation Areas

| Innovation | Description | Benefit |
|------------|-------------|---------|
| **Neural-Aware Compression** | Algorithms designed specifically for neural signal characteristics | 2-3x better compression ratios than generic methods |
| **GPU Acceleration** | CUDA/ROCm optimized kernels for parallel processing | 10-100x faster than CPU-only implementations |
| **Adaptive Selection** | Real-time algorithm selection based on signal properties | Optimal balance of speed, quality, and compression ratio |
| **Streaming Architecture** | Designed for continuous data streams with minimal buffering | Enables real-time BCI applications |
---

## üîÄ Multi-BCI Systems & Electrode Mapping

Different BCI systems use different electrode layouts, channel naming conventions, and sampling characteristics. This project provides a comprehensive adapter layer to make algorithms portable across acquisition systems:

### Supported BCI Devices

| Device | Channels | Sampling Rate | Adapter Status | Use Case |
|--------|----------|---------------|----------------|----------|
| **OpenBCI Cyton** | 8 | 250 Hz | ‚úÖ Complete | Scalp EEG, consumer BCIs |
| **OpenBCI Daisy** | 16 | 250 Hz | ‚úÖ Complete | Multi-channel EEG |
| **Blackrock Neuroport** | 96 | 30 kHz | ‚úÖ Complete | Utah array, intracortical recording |
| **Blackrock Cerebus** | 128 | 30 kHz | ‚úÖ Complete | Dual Utah arrays, high-density recording |
| **Intan RHD2132** | 32 | 20 kHz | ‚úÖ Complete | LFP, research applications |
| **Intan RHD2164** | 64 | 20 kHz | ‚úÖ Complete | Multi-area recording |
| **Intan RHS128** | 128 | 30 kHz | ‚úÖ Complete | Stimulation-capable headstage |
| **Generic HDF5** | Variable | Variable | ‚úÖ Complete | Any HDF5-formatted neural data |

### Adapter Features

- **Electrode mapping**: Declarative JSON/YAML mapping files that translate channel indices and names between systems
- **Resampling adapters**: High-performance polyphase and FFT-based resamplers to normalize sampling rates (250Hz ‚Üî 30kHz)
- **Channel grouping**: Logical grouping for spatial filters and compression (cortical areas, grid rows, functional regions)
- **Calibration metadata**: Store per-session scaling, DC offsets, and bad-channel masks in standardized format
- **Device-specific features**: Utah array grid layouts, headstage type tracking, stimulation capability detection

### Quick Start Examples

#### OpenBCI (Scalp EEG)
```python
from bci_compression.adapters.openbci import OpenBCIAdapter

adapter = OpenBCIAdapter(device='cyton_8ch')
standardized_data = adapter.convert(raw_data)
channel_groups = adapter.get_channel_groups()  # frontal, central, parietal, occipital
```

#### Blackrock (Intracortical)
```python
from bci_compression.adapters.blackrock import BlackrockAdapter

adapter = BlackrockAdapter(device='neuroport_96ch')
downsampled = adapter.resample_to(raw_data, target_rate=1000)
motor_cortex = adapter.get_channel_groups()['motor_cortex']
```

#### Intan (LFP Recording)
```python
from bci_compression.adapters.intan import IntanAdapter

adapter = IntanAdapter(device='rhd2164_64ch')
processed = adapter.convert(raw_data)
has_stim = adapter.stim_capable  # Check stimulation capability
```

#### HDF5 (Generic Loader)
```python
from bci_compression.adapters.hdf5 import HDF5Adapter

adapter = HDF5Adapter.from_hdf5('recording.h5', data_path='/neural/raw')
partial_data = adapter.load_data(start_sample=0, end_sample=10000, channels=[0, 1, 2])
info = adapter.get_info()  # Auto-detect metadata
```

### Multi-Device Pipeline

Combine data from multiple BCI systems in a unified compression pipeline:

```python
from bci_compression.adapters import MultiDevicePipeline

pipeline = MultiDevicePipeline()
pipeline.add_device('openbci', openbci_adapter, priority='normal')   # Scalp EEG
pipeline.add_device('blackrock', blackrock_adapter, priority='high')  # Intracortical (lossless)
pipeline.add_device('intan', intan_adapter, priority='normal')        # LFP

# Process synchronized batch
compressed = pipeline.process_batch({
    'openbci': eeg_data,
    'blackrock': spike_data,
    'intan': lfp_data
})

summary = pipeline.get_summary()  # Get compression statistics
```

### Core Adapter API

The adapter layer exposes a consistent API across all devices:

- `map_channels(data, mapping)` ‚Üí Remap channel indices/names
- `resample(data, src_rate, dst_rate, method='polyphase'|'fft')` ‚Üí Change sampling rate
- `apply_channel_groups(data, groups, reducer='mean')` ‚Üí Apply spatial grouping
- `apply_calibration(data, gains, offsets)` ‚Üí Apply calibration parameters
- `load_mapping_file(filepath)` / `save_mapping_file(mapping, filepath)` ‚Üí I/O utilities

### Example Mapping File (YAML)

```yaml
device: openbci_cyton_8ch
sampling_rate: 250
channels: 8
mapping:
  ch_0: Fp1
  ch_1: Fp2
  ch_2: C3
  ch_3: C4
  ch_4: P7
  ch_5: P8
  ch_6: O1
  ch_7: O2
channel_groups:
  frontal: [0, 1]
  central: [2, 3]
  parietal: [4, 5]
  occipital: [6, 7]
```

### File Locations

- **Adapters**: `src/bci_compression/adapters/`
- **Tests**: `tests/test_adapters.py`, `tests/test_blackrock_adapter.py` (45 tests passing)
- **Examples**: `examples/openbci_adapter_demo.py`, `examples/multi_device_pipeline_example.py`
- **Documentation**: `docs/adapters_guide.md`, `docs/adapters_implementation_summary.md`

### Performance

Real-time streaming compression with <1ms latency:

```python
from examples.streaming_compression_example import StreamingCompressor

compressor = StreamingCompressor(n_channels=8, window_size=1000, overlap=250)
for chunk in data_stream:
    compressed = compressor.process_chunk(chunk)  # ~0.06ms average
```

See `scripts/profile_adapters.py` for detailed performance benchmarks.

## üß™ Testing & Benchmarks Enhancements

We want a fast, reliable test and benchmark workflow that developers can run locally and in CI. Planned improvements include:

- Quick mode tests: fast unit-level smoke tests that run in <30s for quick iteration. These use small synthetic datasets and mock GPU backends when necessary.
- Isolation and timeouts: add per-test timeouts (pytest-timeout) and explicit resource cleanup to prevent hangs. Tests that require longer runtimes live under `tests/long/` and are not part of the `quick` profile.
- Deterministic synthetic data: use fixed random seeds and small synthetic datasets to keep runtimes stable.
- Benchmarks: `scripts/benchmark_runner.py` already supports synthetic and real datasets ‚Äî we'll add `--quick` and `--full` profiles. Quick runs will provide approximate comparisons; full runs will produce CSV/JSON artifacts for analysis.
- Progress reporting: integrate pytest's -q and pytest-benchmark's progress reporting; optionally add a small CLI progress bar in `scripts/benchmark_runner.py` to stream progress.

Suggested test-related changes (I can implement):

1. Add pytest-timeout to `requirements-dev.txt` and apply a 10s timeout to unit tests and 60s to integration/algorithm tests via `pytest.ini`.
2. Mark long-running tests with `@pytest.mark.slow` and put them in `tests/long/`.
3. Add a `tests/quick_run.sh` script that runs the quick profile and exits non-zero on failures.
4. Update `tests/run_tests.py` to support `--profile quick|full|dependencies-only` and ensure `quick` uses smaller data sizes.

These changes will make local development snappier and prevent CI timeouts caused by blocked processes.


## üèóÔ∏è System Architecture

### High-Level Architecture Overview

```mermaid
graph TB
    subgraph "üß† Neural Signal Sources"
        N1[Multi-Channel Neural Arrays<br/>64-256 channels @ 30kHz]
        N2[EMG Sensors<br/>8-32 channels @ 2kHz]
        N3[EEG Electrodes<br/>64-128 channels @ 1kHz]
        N4[Single-Unit Recordings<br/>Spike trains @ variable rate]
    end

    subgraph "‚ö° Real-Time Processing Layer"
        P1[Signal Preprocessing<br/>‚Ä¢ Filtering & Denoising<br/>‚Ä¢ Channel Selection<br/>‚Ä¢ Quality Assessment]
        P2[Feature Extraction<br/>‚Ä¢ Temporal Patterns<br/>‚Ä¢ Frequency Analysis<br/>‚Ä¢ Spatial Correlations]
        P3[Backend Detection<br/>‚Ä¢ GPU Capability Check<br/>‚Ä¢ Performance Profiling<br/>‚Ä¢ Resource Allocation]
    end

    subgraph "üóúÔ∏è Compression Engine Core"
        C1[Algorithm Selection<br/>‚Ä¢ Signal Type Analysis<br/>‚Ä¢ Latency Requirements<br/>‚Ä¢ Quality Constraints]
        C2[Parallel Processing<br/>‚Ä¢ Multi-threaded CPU<br/>‚Ä¢ GPU Acceleration<br/>‚Ä¢ Memory Management]
        C3[Quality Control<br/>‚Ä¢ Compression Validation<br/>‚Ä¢ Error Detection<br/>‚Ä¢ Adaptive Tuning]
    end

    subgraph "üéØ Output & Applications"
        A1[Real-time Control<br/>‚Ä¢ Prosthetic Devices<br/>‚Ä¢ Robotic Systems<br/>‚Ä¢ Gaming Interfaces]
        A2[Data Storage<br/>‚Ä¢ HDF5 Archives<br/>‚Ä¢ Cloud Storage<br/>‚Ä¢ Local Databases]
        A3[Analytics Pipeline<br/>‚Ä¢ Machine Learning<br/>‚Ä¢ Statistical Analysis<br/>‚Ä¢ Visualization]
        A4[Streaming Services<br/>‚Ä¢ WebRTC Transmission<br/>‚Ä¢ Mobile Apps<br/>‚Ä¢ Remote Monitoring]
    end

    N1 --> P1
    N2 --> P1
    N3 --> P1
    N4 --> P1

    P1 --> P2
    P2 --> P3
    P3 --> C1

    C1 --> C2
    C2 --> C3

    C3 --> A1
    C3 --> A2
    C3 --> A3
    C3 --> A4

    classDef neuralsource fill:#1a365d,stroke:#2c5282,stroke-width:2px,color:#ffffff
    classDef processing fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
    classDef compression fill:#744210,stroke:#975a16,stroke-width:2px,color:#ffffff
    classDef output fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

    class N1,N2,N3,N4 neuralsource
    class P1,P2,P3 processing
    class C1,C2,C3 compression
    class A1,A2,A3,A4 output
```

### GPU Acceleration Architecture

```mermaid
graph LR
    subgraph "üíª Host System"
        CPU[CPU Controller<br/>‚Ä¢ Task Scheduling<br/>‚Ä¢ Memory Management<br/>‚Ä¢ I/O Operations]
        RAM[System Memory<br/>‚Ä¢ Input Buffers<br/>‚Ä¢ Algorithm Storage<br/>‚Ä¢ Result Cache]
    end

    subgraph "üéÆ GPU Processing Units"
        CUDA[CUDA Cores<br/>‚Ä¢ Parallel Compression<br/>‚Ä¢ Matrix Operations<br/>‚Ä¢ Stream Processing]
        ROCm[ROCm Compute<br/>‚Ä¢ AMD GPU Support<br/>‚Ä¢ HIP Kernels<br/>‚Ä¢ Memory Coalescing]
        MEM[GPU Memory<br/>‚Ä¢ High Bandwidth<br/>‚Ä¢ Shared Buffers<br/>‚Ä¢ Texture Cache]
    end

    subgraph "‚öôÔ∏è Acceleration Backend"
        DETECT[Backend Detection<br/>‚Ä¢ Hardware Enumeration<br/>‚Ä¢ Capability Testing<br/>‚Ä¢ Performance Profiling]
        SCHED[Work Scheduler<br/>‚Ä¢ Load Balancing<br/>‚Ä¢ Memory Allocation<br/>‚Ä¢ Error Handling]
        OPTIM[Performance Optimization<br/>‚Ä¢ Kernel Tuning<br/>‚Ä¢ Memory Access Patterns<br/>‚Ä¢ Pipeline Efficiency]
    end

    CPU --> DETECT
    RAM --> DETECT

    DETECT --> CUDA
    DETECT --> ROCm

    CUDA --> SCHED
    ROCm --> SCHED
    MEM --> SCHED

    SCHED --> OPTIM
    OPTIM --> CPU

    classDef host fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
    classDef gpu fill:#1a365d,stroke:#2c5282,stroke-width:2px,color:#ffffff
    classDef backend fill:#744210,stroke:#975a16,stroke-width:2px,color:#ffffff

    class CPU,RAM host
    class CUDA,ROCm,MEM gpu
    class DETECT,SCHED,OPTIM backend
```

### Data Flow Pipeline

```mermaid
flowchart TD
    START([Neural Data Input<br/>Multi-channel streams]) --> PREPROCESS{Signal Preprocessing}

    PREPROCESS --> ANALYZE[Signal Analysis<br/>‚Ä¢ Type Classification<br/>‚Ä¢ Quality Assessment<br/>‚Ä¢ Resource Requirements]

    ANALYZE --> BACKEND{Backend Selection}

    BACKEND -->|High Performance| GPU_PATH[GPU Acceleration Path<br/>‚Ä¢ CUDA/ROCm Kernels<br/>‚Ä¢ Parallel Processing<br/>‚Ä¢ Memory Optimization]

    BACKEND -->|Compatibility| CPU_PATH[CPU Processing Path<br/>‚Ä¢ Multi-threading<br/>‚Ä¢ SIMD Instructions<br/>‚Ä¢ Cache Optimization]

    GPU_PATH --> ALGORITHM{Algorithm Selection}
    CPU_PATH --> ALGORITHM

    ALGORITHM -->|Ultra-Fast| LZ4[LZ4 Compression<br/>< 0.1ms latency]
    ALGORITHM -->|Balanced| ZSTD[Zstandard<br/>< 1ms latency]
    ALGORITHM -->|High-Ratio| NEURAL[Neural Algorithms<br/>< 2ms latency]

    LZ4 --> VALIDATE{Quality Validation}
    ZSTD --> VALIDATE
    NEURAL --> VALIDATE

    VALIDATE -->|Pass| OUTPUT[Compressed Output<br/>‚Ä¢ Streaming Ready<br/>‚Ä¢ Metadata Attached<br/>‚Ä¢ Error Corrected]
    VALIDATE -->|Fail| FALLBACK[Fallback Algorithm<br/>‚Ä¢ Conservative Settings<br/>‚Ä¢ Guaranteed Quality]

    FALLBACK --> OUTPUT
    OUTPUT --> END([Application Layer<br/>Real-time usage])

    classDef process fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
    classDef decision fill:#1a365d,stroke:#2c5282,stroke-width:2px,color:#ffffff
    classDef algorithm fill:#744210,stroke:#975a16,stroke-width:2px,color:#ffffff
    classDef endpoint fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

    class PREPROCESS,ANALYZE,GPU_PATH,CPU_PATH,VALIDATE,FALLBACK process
    class BACKEND,ALGORITHM decision
    class LZ4,ZSTD,NEURAL algorithm
    class START,OUTPUT,END endpoint
```

### Complete Technology Ecosystem

```mermaid
graph TB
    subgraph "üé® Frontend Layer"
        WEB[Web Dashboard<br/>React + TypeScript<br/>Real-time Visualization]
        API_DOCS[API Documentation<br/>Swagger/OpenAPI<br/>Interactive Testing]
    end

    subgraph "üîå API Layer"
        FASTAPI[FastAPI Server<br/>Python 3.8+<br/>Async/Await]
        PYDANTIC[Pydantic Models<br/>Type Validation<br/>Schema Generation]
        UVICORN[Uvicorn Server<br/>ASGI Protocol<br/>WebSocket Support]
    end

    subgraph "üßÆ Core Processing"
        COMPRESSION[Compression Engine<br/>Multi-Algorithm Support<br/>Adaptive Selection]
        SIGNAL[Signal Processing<br/>SciPy/NumPy<br/>Filtering & Analysis]
        STREAMING[Streaming Pipeline<br/>Chunk Processing<br/>Real-time Flow]
    end

    subgraph "‚ö° Acceleration Layer"
        GPU_BACKEND[GPU Backend<br/>CUDA/ROCm Detection<br/>Memory Management]
        CUPY[CuPy Arrays<br/>GPU NumPy<br/>Zero-copy Transfers]
        PYTORCH[PyTorch<br/>Deep Learning<br/>AI Compression]
    end

    subgraph "üì¶ Compression Algorithms"
        TRADITIONAL[Traditional<br/>LZ4, Zstandard, Blosc<br/>< 1ms latency]
        NEURAL_ALG[Neural-Optimized<br/>Wavelet, Quantization<br/>5-10x compression]
        AI_ALG[AI-Powered<br/>Autoencoders, Transformers<br/>15-40x compression]
    end

    subgraph "üíæ Storage & Data"
        HDF5[HDF5 Archives<br/>Hierarchical Storage<br/>Metadata Support]
        FORMATS[Neural Formats<br/>NEV, NSx, Intan<br/>Format Converters]
        CACHE[Result Cache<br/>Redis/Memory<br/>Fast Retrieval]
    end

    subgraph "üìä Monitoring & Ops"
        PROMETHEUS[Prometheus Metrics<br/>Performance Tracking<br/>Time-series Data]
        LOGGING[Structured Logging<br/>JSON Format<br/>Error Tracking]
        PROFILING[Performance Profiling<br/>Latency Analysis<br/>Resource Usage]
    end

    subgraph "üê≥ Deployment"
        DOCKER[Docker Containers<br/>Multi-stage Builds<br/>CPU/CUDA/ROCm]
        COMPOSE[Docker Compose<br/>Service Orchestration<br/>Development Profiles]
        K8S[Kubernetes<br/>Production Scale<br/>Auto-scaling]
    end

    subgraph "üß™ Testing & Quality"
        PYTEST[Pytest Framework<br/>Unit & Integration<br/>Coverage Reports]
        BENCHMARK[Benchmarking Suite<br/>Performance Testing<br/>Regression Detection]
        CI_CD[GitHub Actions<br/>CI/CD Pipeline<br/>Automated Testing]
    end

    WEB --> FASTAPI
    API_DOCS --> FASTAPI
    FASTAPI --> PYDANTIC
    FASTAPI --> UVICORN

    FASTAPI --> COMPRESSION
    COMPRESSION --> SIGNAL
    COMPRESSION --> STREAMING

    COMPRESSION --> GPU_BACKEND
    GPU_BACKEND --> CUPY
    GPU_BACKEND --> PYTORCH

    COMPRESSION --> TRADITIONAL
    COMPRESSION --> NEURAL_ALG
    COMPRESSION --> AI_ALG

    STREAMING --> HDF5
    STREAMING --> FORMATS
    COMPRESSION --> CACHE

    FASTAPI --> PROMETHEUS
    FASTAPI --> LOGGING
    GPU_BACKEND --> PROFILING

    DOCKER --> FASTAPI
    COMPOSE --> DOCKER
    K8S --> DOCKER

    PYTEST --> COMPRESSION
    BENCHMARK --> COMPRESSION
    CI_CD --> PYTEST
    CI_CD --> BENCHMARK

    classDef frontend fill:#1a365d,stroke:#2c5282,stroke-width:2px,color:#ffffff
    classDef api fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
    classDef core fill:#744210,stroke:#975a16,stroke-width:2px,color:#ffffff
    classDef gpu fill:#1e3a8a,stroke:#1e40af,stroke-width:2px,color:#ffffff
    classDef algorithms fill:#065f46,stroke:#047857,stroke-width:2px,color:#ffffff
    classDef storage fill:#7c2d12,stroke:#9a3412,stroke-width:2px,color:#ffffff
    classDef monitoring fill:#4c1d95,stroke:#5b21b6,stroke-width:2px,color:#ffffff
    classDef deployment fill:#831843,stroke:#9f1239,stroke-width:2px,color:#ffffff
    classDef testing fill:#14532d,stroke:#166534,stroke-width:2px,color:#ffffff

    class WEB,API_DOCS frontend
    class FASTAPI,PYDANTIC,UVICORN api
    class COMPRESSION,SIGNAL,STREAMING core
    class GPU_BACKEND,CUPY,PYTORCH gpu
    class TRADITIONAL,NEURAL_ALG,AI_ALG algorithms
    class HDF5,FORMATS,CACHE storage
    class PROMETHEUS,LOGGING,PROFILING monitoring
    class DOCKER,COMPOSE,K8S deployment
    class PYTEST,BENCHMARK,CI_CD testing
```

**Technology Rationale Summary:**

| Layer | Key Technologies | Why This Combination |
|-------|-----------------|---------------------|
| **Frontend** | React + TypeScript | Type safety, component reusability, real-time updates |
| **API** | FastAPI + Pydantic | Automatic docs, type validation, high performance |
| **Core** | NumPy + SciPy | Scientific computing standard, optimized algorithms |
| **GPU** | CUDA + ROCm + CuPy | Broad GPU support, minimal code changes |
| **Algorithms** | LZ4 + Zstandard + AI | Speed/ratio trade-offs, neural-specific optimization |
| **Storage** | HDF5 | Scientific data standard, efficient compression |
| **Monitoring** | Prometheus + JSON logs | Industry standard, powerful querying |
| **Deployment** | Docker + K8s | Reproducibility, scalability, platform independence |
| **Testing** | Pytest + Benchmarks | Comprehensive coverage, performance tracking |

---

## üöÄ Quick Start

### Prerequisites

| Requirement | Version | Purpose | Installation |
|-------------|---------|---------|--------------|
| **Python** | 3.8+ | Core runtime environment | [Download Python](https://python.org/downloads) |
| **Docker** | 20.10+ | Containerized deployment | [Install Docker](https://docs.docker.com/get-docker/) |
| **GPU Drivers** | Latest | Hardware acceleration | [NVIDIA](https://developer.nvidia.com/cuda-downloads) \| [AMD](https://rocmdocs.amd.com/en/latest/Installation_Guide/Installation-Guide.html) |
| **Git** | 2.25+ | Version control | [Install Git](https://git-scm.com/downloads) |

### Installation & Setup

#### Option 1: Automated Setup (Recommended)

```bash
# Clone the repository
git clone https://github.com/hkevin01/brain-computer-compression.git
cd brain-computer-compression

# One-command setup with development environment
make setup

# Start all services with auto-detected GPU backend
./run.sh up

# Check system status and capabilities
./run.sh status

# Open interactive API documentation
./run.sh gui:open
```

#### Option 2: Manual Development Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install with development dependencies
pip install -e ".[dev,quality]"

# Install GPU acceleration (optional)
pip install -e ".[cuda]"  # For NVIDIA GPUs
# pip install -e ".[rocm]"  # For AMD GPUs

# Start API server
python -m bci_compression.api.server

# In another terminal, start dashboard
python -m http.server 3000 --directory web
```

### Verification

```bash
# Run health checks
./run.sh health

# Execute benchmarks
./run.sh bench:all

# Run test suite
make test

# Check code quality
make lint
```

---

## üîß Technology Stack

### Core Technologies

| Technology | Version | Purpose | Why Chosen |
|------------|---------|---------|------------|
| **Python** | 3.8-3.12 | Primary language | ‚Ä¢ Excellent scientific computing ecosystem<br/>‚Ä¢ Rich neural data processing libraries<br/>‚Ä¢ Easy integration with ML frameworks |
| **NumPy** | 1.21+ | Numerical computing | ‚Ä¢ Optimized array operations for neural data<br/>‚Ä¢ Memory-efficient multi-dimensional arrays<br/>‚Ä¢ Foundation for scientific Python stack |
| **SciPy** | 1.7+ | Scientific algorithms | ‚Ä¢ Signal processing functions (filters, FFT)<br/>‚Ä¢ Statistical analysis for neural patterns<br/>‚Ä¢ Optimized implementations of math functions |
| **PyTorch** | 1.13+ | Machine learning | ‚Ä¢ GPU acceleration for neural networks<br/>‚Ä¢ Dynamic computation graphs<br/>‚Ä¢ Strong ecosystem for research |

### GPU Acceleration

| Technology | Purpose | Implementation | Benefits |
|------------|---------|----------------|----------|
| **CUDA 12.x** | NVIDIA GPU support | CuPy integration + custom kernels | ‚Ä¢ 10-100x speedup for parallel operations<br/>‚Ä¢ Mature ecosystem with extensive libraries<br/>‚Ä¢ Optimized memory management |
| **ROCm 6.x** | AMD GPU support | HIP kernels + PyTorch backend | ‚Ä¢ Open-source alternative to CUDA<br/>‚Ä¢ Growing support for scientific computing<br/>‚Ä¢ Better price/performance for some workloads |
| **CuPy** | GPU-accelerated NumPy | Drop-in replacement for NumPy | ‚Ä¢ Minimal code changes for GPU acceleration<br/>‚Ä¢ Automatic memory management<br/>‚Ä¢ Seamless CPU-GPU transfers |

### Web & API Framework

| Component | Technology | Purpose | Why Chosen |
|-----------|------------|---------|------------|
| **FastAPI** | Modern Python web framework | RESTful API server | ‚Ä¢ Automatic API documentation<br/>‚Ä¢ Type validation and serialization<br/>‚Ä¢ High performance (comparable to Node.js)<br/>‚Ä¢ Built-in async support |
| **Pydantic** | Data validation | Request/response models | ‚Ä¢ Runtime type checking<br/>‚Ä¢ Automatic JSON serialization<br/>‚Ä¢ Clear error messages<br/>‚Ä¢ Integration with FastAPI |
| **Uvicorn** | ASGI server | Production deployment | ‚Ä¢ High-performance async server<br/>‚Ä¢ Hot reloading for development<br/>‚Ä¢ WebSocket support for streaming |

### Containerization & Orchestration

| Technology | Purpose | Configuration | Benefits |
|------------|---------|---------------|----------|
| **Docker** | Application containerization | Multi-stage builds | ‚Ä¢ Consistent environments across platforms<br/>‚Ä¢ Isolated dependencies<br/>‚Ä¢ Easy deployment and scaling |
| **Docker Compose** | Service orchestration | Profile-based configs | ‚Ä¢ Multi-service coordination<br/>‚Ä¢ Environment-specific configurations<br/>‚Ä¢ Development vs production profiles |
| **Multi-stage Builds** | Optimized images | CPU/CUDA/ROCm variants | ‚Ä¢ Smaller production images<br/>‚Ä¢ Backend-specific optimizations<br/>‚Ä¢ Reduced attack surface |

### Development & Quality Tools

| Category | Tools | Purpose | Integration |
|----------|-------|---------|-------------|
| **Code Quality** | Ruff, Black, MyPy | Linting, formatting, type checking | Pre-commit hooks + CI/CD |
| **Testing** | Pytest, Hypothesis | Unit tests, property-based testing | Automated test discovery |
| **Benchmarking** | pytest-benchmark | Performance measurement | Integrated with test suite |
| **Documentation** | Sphinx, MkDocs | API docs, user guides | Auto-generated from docstrings |

### Data Storage & Formats

| Technology | Use Case | Features | Why Chosen |
|------------|----------|----------|------------|
| **HDF5** | Neural data archives | Hierarchical, compressed | ‚Ä¢ Industry standard for scientific data<br/>‚Ä¢ Built-in compression<br/>‚Ä¢ Metadata support<br/>‚Ä¢ Cross-platform compatibility |
| **JSON** | Configuration, API | Human-readable, structured | ‚Ä¢ Universal support<br/>‚Ä¢ Easy debugging<br/>‚Ä¢ Schema validation with Pydantic |
| **MessagePack** | Binary serialization | Compact, fast | ‚Ä¢ Smaller than JSON<br/>‚Ä¢ Faster parsing<br/>‚Ä¢ Maintains type information |

### Compression Libraries

| Library | Purpose | Performance | Integration |
|---------|---------|-------------|-------------|
| **LZ4** | Ultra-fast compression | < 0.1ms latency | Direct Python bindings |
| **Zstandard** | Balanced compression | < 1ms latency | Facebook's library with Python API |
| **Blosc** | Array compression | Optimized for NumPy | Native multi-threading support |
| **PyWavelets** | Wavelet transforms | Scientific-grade | SciPy ecosystem integration |

---

## ‚ö° GPU Acceleration

### Backend Detection & Selection

The toolkit automatically detects and optimizes for available hardware:

```mermaid
flowchart TD
    START([System Startup]) --> DETECT{Hardware Detection}

    DETECT -->|NVIDIA GPU Found| CUDA_CHECK[CUDA Capability Check<br/>‚Ä¢ Driver Version<br/>‚Ä¢ Compute Capability<br/>‚Ä¢ Memory Available]

    DETECT -->|AMD GPU Found| ROCM_CHECK[ROCm Capability Check<br/>‚Ä¢ ROCm Version<br/>‚Ä¢ HIP Support<br/>‚Ä¢ Memory Available]

    DETECT -->|CPU Only| CPU_OPT[CPU Optimization<br/>‚Ä¢ Thread Count<br/>‚Ä¢ SIMD Support<br/>‚Ä¢ Cache Optimization]

    CUDA_CHECK -->|Compatible| CUDA_INIT[CUDA Backend<br/>‚Ä¢ CuPy Arrays<br/>‚Ä¢ Custom Kernels<br/>‚Ä¢ Memory Pools]

    ROCM_CHECK -->|Compatible| ROCM_INIT[ROCm Backend<br/>‚Ä¢ HIP Kernels<br/>‚Ä¢ PyTorch Backend<br/>‚Ä¢ Unified Memory]

    CPU_OPT --> CPU_INIT[CPU Backend<br/>‚Ä¢ NumPy + BLAS<br/>‚Ä¢ Multi-threading<br/>‚Ä¢ Memory Mapping]

    CUDA_CHECK -->|Incompatible| CPU_INIT
    ROCM_CHECK -->|Incompatible| CPU_INIT

    CUDA_INIT --> READY[Backend Ready]
    ROCM_INIT --> READY
    CPU_INIT --> READY

    READY --> BENCHMARK[Performance Profiling<br/>‚Ä¢ Throughput Testing<br/>‚Ä¢ Latency Measurement<br/>‚Ä¢ Memory Bandwidth]

    BENCHMARK --> OPTIMIZE[Runtime Optimization<br/>‚Ä¢ Kernel Tuning<br/>‚Ä¢ Memory Layout<br/>‚Ä¢ Pipeline Depth]

    classDef detection fill:#1a365d,stroke:#2c5282,stroke-width:2px,color:#ffffff
    classDef backend fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
    classDef optimization fill:#744210,stroke:#975a16,stroke-width:2px,color:#ffffff

    class DETECT,CUDA_CHECK,ROCM_CHECK detection
    class CUDA_INIT,ROCM_INIT,CPU_INIT,READY backend
    class BENCHMARK,OPTIMIZE optimization
```

### Performance Optimization Strategies

| Strategy | Implementation | Benefit | Use Case |
|----------|----------------|---------|----------|
| **Memory Coalescing** | Aligned memory access patterns | 2-10x bandwidth improvement | Large array operations |
| **Stream Processing** | Overlapped compute and memory | Reduced latency, higher throughput | Real-time streaming |
| **Kernel Fusion** | Combined operations in single kernel | Reduced memory overhead | Complex transformations |
| **Adaptive Block Size** | Dynamic workload partitioning | Optimal GPU utilization | Variable input sizes |

### Hardware Requirements & Performance

| GPU Tier | Examples | Expected Performance | Supported Features |
|----------|----------|---------------------|-------------------|
| **High-End** | RTX 4090, A100, MI300X | > 1000 MB/s throughput | All algorithms, maximum parallelism |
| **Mid-Range** | RTX 3060, RX 6600 XT | 200-500 MB/s throughput | Most algorithms, good parallelism |
| **Entry-Level** | GTX 1660, RX 5500 XT | 50-200 MB/s throughput | Basic algorithms, limited parallelism |
| **CPU Fallback** | Any modern CPU | 10-50 MB/s throughput | All algorithms, multi-threading |

---

## üóúÔ∏è Compression Algorithms

### Algorithm Categories & Selection

```mermaid
graph TD
    INPUT[Neural Data Input<br/>Multi-channel streams] --> ANALYSIS{Signal Analysis}

    ANALYSIS --> TYPE{Signal Type}
    TYPE -->|Continuous EEG/LFP| CONT[Continuous Signals<br/>High temporal resolution]
    TYPE -->|Spike Trains| SPIKE[Event-Based Signals<br/>Sparse temporal data]
    TYPE -->|EMG/Muscular| EMG[Physiological Signals<br/>Variable amplitude]

    ANALYSIS --> QUALITY{Quality Requirements}
    QUALITY -->|Research Grade| LOSSLESS[Lossless Algorithms<br/>Perfect reconstruction]
    QUALITY -->|Clinical| NEARLOS[Near-Lossless<br/>Perceptually identical]
    QUALITY -->|Monitoring| LOSSY[Lossy Algorithms<br/>Feature preservation]

    ANALYSIS --> LATENCY{Latency Constraints}
    LATENCY -->|Real-time Control| ULTRA[Ultra-Fast<br/>< 0.1ms latency]
    LATENCY -->|Interactive| FAST[Fast<br/>< 1ms latency]
    LATENCY -->|Batch Processing| OPTIMAL[Optimal Ratio<br/>< 2ms latency]

    CONT --> LZ4_CONT[LZ4 + Preprocessing]
    SPIKE --> SPIKE_CODEC[Spike Codec]
    EMG --> BLOSC_EMG[Blosc + Filtering]

    LOSSLESS --> ZSTD_LOSS[Zstandard]
    NEARLOS --> NEURAL_NEAR[Neural LZ77]
    LOSSY --> TRANSFORM[Transformer Models]

    ULTRA --> LZ4_ULTRA[LZ4]
    FAST --> ZSTD_FAST[Zstandard]
    OPTIMAL --> AI_OPT[AI Models]

    classDef input fill:#1a365d,stroke:#2c5282,stroke-width:2px,color:#ffffff
    classDef analysis fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
    classDef algorithm fill:#744210,stroke:#975a16,stroke-width:2px,color:#ffffff

    class INPUT input
    class ANALYSIS,TYPE,QUALITY,LATENCY analysis
    class LZ4_CONT,SPIKE_CODEC,BLOSC_EMG,ZSTD_LOSS,NEURAL_NEAR,TRANSFORM,LZ4_ULTRA,ZSTD_FAST,AI_OPT algorithm
```

### Traditional Compression Algorithms

#### LZ4 - Ultra-Fast Real-Time Compression

**Purpose**: Absolute minimum latency for real-time BCI control applications

| Metric | Performance | Use Case |
|--------|-------------|----------|
| **Latency** | < 0.1ms | Prosthetic control, gaming interfaces |
| **Compression Ratio** | 1.5-2.5x | Moderate compression, high speed priority |
| **Throughput** | > 500 MB/s | Continuous neural streaming |
| **Memory Usage** | Very Low | Embedded BCI systems |

**Technical Details**:
- **Algorithm Type**: Dictionary-based LZ77 variant with fast parsing
- **Implementation**: Optimized C library with Python bindings
- **GPU Acceleration**: Custom CUDA kernels for parallel block processing
- **Neural Data Optimization**: Preprocessor for temporal correlation detection

#### Zstandard (ZSTD) - Intelligent Dictionary Compression

**Purpose**: Balanced performance for most neural data processing scenarios

| Metric | Performance | Use Case |
|--------|-------------|----------|
| **Latency** | < 1ms | Real-time analysis, data logging |
| **Compression Ratio** | 3-6x | Good balance of speed and compression |
| **Throughput** | 100-300 MB/s | Multi-channel recordings |
| **Memory Usage** | Moderate | Standard workstation deployment |

**Technical Details**:
- **Algorithm Type**: Advanced dictionary compression with entropy coding
- **Implementation**: Facebook's reference implementation with neural adaptations
- **GPU Acceleration**: Parallel dictionary construction and entropy encoding
- **Neural Data Optimization**: Pre-trained dictionaries for common neural patterns

#### Blosc - Multi-Dimensional Array Specialist

**Purpose**: Optimized for multi-channel neural array data with spatial correlations

| Metric | Performance | Use Case |
|--------|-------------|----------|
| **Latency** | < 0.5ms | Array-based recordings (Utah arrays, ECoG) |
| **Compression Ratio** | 4-8x | Excellent for structured neural data |
| **Throughput** | 200-400 MB/s | High-density electrode arrays |
| **Memory Usage** | Low | Memory-efficient streaming |

**Technical Details**:
- **Algorithm Type**: Chunked compression with multiple algorithms (LZ4, ZSTD, ZLIB)
- **Implementation**: Optimized for NumPy arrays with multi-threading
- **GPU Acceleration**: Parallel chunk processing and memory coalescing
- **Neural Data Optimization**: Spatial correlation detection across channels

### Neural-Optimized Algorithms

#### Neural LZ77 - BCI-Optimized Temporal Compression

**Purpose**: Leverages temporal patterns specific to neural signals

- **Innovation**: Pattern recognition for neural oscillations and spike timing
- **Performance**: 5-10x compression with <1ms latency
- **Specialization**: Optimized for neural frequency bands and temporal structure
- **Implementation**: Custom algorithm with GPU-accelerated pattern matching

#### Perceptual Quantization - Neural Feature Preservation

**Purpose**: Lossy compression that preserves neural decoding performance

- **Innovation**: Quantization based on neural feature importance
- **Performance**: 10-20x compression with minimal decoding accuracy loss
- **Specialization**: Preserves signal features critical for BCI applications
- **Implementation**: Learned quantization levels from neural decoding tasks

#### Adaptive Wavelets - Multi-Resolution Neural Analysis

**Purpose**: Time-frequency decomposition optimized for neural oscillations

- **Innovation**: Adaptive wavelet bases learned from neural data
- **Performance**: 8-15x compression with frequency-specific quality control
- **Specialization**: Preserves power spectral density and phase relationships
- **Implementation**: GPU-accelerated wavelet transforms with learned bases

### AI-Powered Compression

#### Deep Autoencoders - Learned Neural Representations

**Purpose**: End-to-end learned compression optimized for neural data

| Component | Architecture | Innovation |
|-----------|--------------|------------|
| **Encoder** | 1D CNN + LSTM | Captures temporal dependencies |
| **Bottleneck** | Learned compression | Adaptive rate control |
| **Decoder** | Transposed CNN | Reconstruction optimization |
| **Training** | Neural data corpus | Domain-specific learning |

**Performance**:
- **Compression Ratio**: 15-30x depending on signal type
- **Latency**: 1-5ms (GPU required)
- **Quality**: Perceptually lossless for most BCI applications
- **Adaptability**: Continuously improves with more neural data

#### Transformer Models - Attention-Based Temporal Patterns

**Purpose**: Captures long-range temporal dependencies in neural signals

| Component | Architecture | Purpose |
|-----------|--------------|---------|
| **Positional Encoding** | Sinusoidal + learned | Temporal position awareness |
| **Multi-Head Attention** | 8-16 heads | Parallel pattern recognition |
| **Feed-Forward** | Gated linear units | Non-linear transformations |
| **Compression Head** | Learned quantization | Rate-distortion optimization |

**Performance**:
- **Compression Ratio**: 20-40x with quality control
- **Latency**: 2-10ms (requires high-end GPU)
- **Quality**: State-of-the-art for complex neural patterns
- **Scalability**: Handles variable-length sequences efficiently

#### Variational Autoencoders (VAE) - Probabilistic Quality Control

**Purpose**: Provides uncertainty estimates and quality guarantees

| Component | Function | Benefit |
|-----------|----------|---------|
| **Probabilistic Encoder** | Uncertainty quantification | Quality assessment |
| **Latent Space** | Structured representation | Interpretable compression |
| **Decoder** | Reconstruction + uncertainty | Error bounds |
| **Rate Control** | Adaptive bitrate | Quality-based allocation |

**Performance**:
- **Compression Ratio**: 10-25x with quality bounds
- **Latency**: 3-8ms (GPU recommended)
- **Quality**: Provides confidence intervals for reconstruction
- **Reliability**: Built-in quality assessment and error detection

### Performance Characteristics

#### Real-Time Processing Guarantees

| Algorithm Class | Worst-Case Latency | Throughput | Memory | Use Case |
|-----------------|-------------------|------------|--------|----------|
| **Ultra-Fast** | < 0.1ms | > 500 MB/s | < 10MB | Real-time control |
| **Balanced** | < 1ms | 100-500 MB/s | 10-50MB | General purpose |
| **High-Ratio** | < 2ms | 50-200 MB/s | 50-200MB | Storage/transmission |
| **AI-Powered** | < 10ms | 20-100 MB/s | 200MB-2GB | Research/analysis |

#### Hardware Acceleration Benefits

| Hardware | Speedup vs CPU | Supported Algorithms | Optimal Use Cases |
|----------|----------------|---------------------|-------------------|
| **High-End GPU** | 50-100x | All algorithms | Real-time + AI compression |
| **Mid-Range GPU** | 20-50x | Traditional + some AI | Balanced workloads |
| **Entry GPU** | 5-20x | Traditional algorithms | Cost-effective acceleration |
| **Multi-Core CPU** | 1-4x | All algorithms | Compatibility fallback |

#### Memory Efficiency

| Optimization | Technique | Benefit | Implementation |
|--------------|-----------|---------|----------------|
| **Streaming** | Chunk-based processing | Constant memory usage | Sliding window buffers |
| **In-Place** | No intermediate copies | 50% memory reduction | Zero-copy operations |
| **Memory Pools** | Pre-allocated buffers | Reduced allocation overhead | GPU memory management |
| **Compression Caching** | LRU cache for patterns | Faster repeated patterns | Dictionary reuse |

## üìÅ Project Structure

```
brain-computer-compression/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ requirements*.txt            # Python dependencies
‚îú‚îÄ‚îÄ pyproject.toml              # Python project config
‚îú‚îÄ‚îÄ run.sh                      # Main orchestration script
‚îú‚îÄ‚îÄ docs/                       # üìö Documentation
‚îÇ   ‚îú‚îÄ‚îÄ guides/                 # User guides
‚îÇ   ‚îî‚îÄ‚îÄ project/               # Project documentation
‚îú‚îÄ‚îÄ docker/                     # üê≥ Docker configuration
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile             # Main backend image
‚îÇ   ‚îî‚îÄ‚îÄ compose/               # Docker compose files
‚îú‚îÄ‚îÄ scripts/                    # üîß Scripts and tools
‚îÇ   ‚îú‚îÄ‚îÄ setup/                 # Installation scripts
‚îÇ   ‚îî‚îÄ‚îÄ tools/                 # Utility scripts
‚îú‚îÄ‚îÄ src/                       # üß† Core source code
‚îú‚îÄ‚îÄ tests/                     # üß™ Test suite
‚îú‚îÄ‚îÄ dashboard/                 # üåê React GUI
‚îú‚îÄ‚îÄ examples/                  # üìñ Usage examples
‚îî‚îÄ‚îÄ notebooks/                 # üìä Jupyter notebooks
```

## ÔøΩ Examples & Demos

### Device Adapter Examples

Comprehensive examples demonstrating BCI device integration:

| Example | File | Description | Features |
|---------|------|-------------|----------|
| **OpenBCI Demo** | `examples/openbci_adapter_demo.py` | 6 scenarios for OpenBCI devices | Basic conversion, resampling, channel grouping, calibration, full pipeline, multi-device |
| **Streaming Compression** | `examples/streaming_compression_example.py` | Real-time streaming with <1ms latency | Sliding windows, circular buffers, latency monitoring, throughput stats |
| **Multi-Device Pipeline** | `examples/multi_device_pipeline_example.py` | Unified pipeline for multiple BCI systems | OpenBCI + Blackrock + Intan integration, hierarchical compression, channel alignment |

### Running Examples

```bash
# OpenBCI adapter demos (all 6 scenarios)
python examples/openbci_adapter_demo.py

# Real-time streaming compression
python examples/streaming_compression_example.py

# Multi-device integration
python examples/multi_device_pipeline_example.py

# EMG signal processing
python examples/emg_demo.py

# Transformer-based compression
python examples/transformer_demo.py

# GPU-accelerated compression (Jupyter notebook)
jupyter lab examples/cuda_acceleration.ipynb
```

### Performance Profiling

```bash
# Profile all adapters with detailed benchmarks
python scripts/profile_adapters.py

# View generated profiling report
cat results/adapter_profiling_report.txt
```

**Expected Performance:**
- OpenBCI: 0.059ms per window (169,948k samples/s)
- Blackrock: 4.216ms per window (7,116k samples/s)
- Intan: 1.803ms per window (11,090k samples/s)
- Streaming: <0.1ms average latency, <0.13ms max

### Jupyter Notebooks

Interactive analysis and visualization:

```bash
# Start Jupyter Lab
jupyter lab

# Or use the provided task
./run.sh jupyter
```

Available notebooks:
- `notebooks/compression_analysis.ipynb` - Compression algorithm comparison
- `notebooks/benchmarking_results.ipynb` - Performance analysis
- `examples/cuda_acceleration.ipynb` - GPU acceleration demos

## ÔøΩüìö Documentation

- **[Quick Start Guide](docs/guides/DOCKER_QUICK_START.md)** - Get started with Docker
- **[Adapters Guide](docs/adapters_guide.md)** - Complete BCI adapter documentation
- **[Adapters Implementation](docs/adapters_implementation_summary.md)** - Technical implementation details
- **[Docker Troubleshooting](docs/guides/DOCKER_BUILD_FIX.md)** - Fix common Docker issues
- **[Contributing Guide](docs/CONTRIBUTING.md)** - How to contribute
- **[Changelog](docs/CHANGELOG.md)** - Version history
- **[Project Status](docs/project/STATUS_REPORT.md)** - Current development status

## üê≥ Docker Usage - Zero Configuration Required

**Docker-First Design Benefits:**

- üöÄ **Instant Setup**: One command starts everything
- üîí **Isolated Environment**: No conflicts with system packages
- üì¶ **Batteries Included**: All dependencies pre-configured
- üîÑ **Consistent Results**: Same environment across all systems
- üõ°Ô∏è **Error-Free**: Template generation prevents configuration mistakes

All Docker files are organized in the `docker/` directory:

```bash
# Build images (optional - auto-built on first run)
./run.sh build

# Start services - everything you need!
./run.sh up

# View logs
./run.sh logs

# Stop services
./run.sh down
```

## üîß Development Tools

Utility scripts are in `scripts/tools/`:

- **Setup**: `scripts/setup/setup.sh` - Quick environment setup
- **Docker Tools**: `scripts/tools/test_docker_build.sh` - Test Docker builds
- **Cleanup**: `scripts/tools/cleanup_now.sh` - Clean temporary files

## ‚ú® Key Features

### üîå Multi-BCI System Support

**üß† Native Support for 8+ BCI Systems** ‚ú® NEW

Full adapter implementations with tested, production-ready code:

| System | Channels | Sampling | Status | Implementation |
|--------|----------|----------|--------|----------------|
| **OpenBCI Cyton/Daisy** | 8-16 | 250 Hz | ‚úÖ Complete | Full adapter with electrode mapping |
| **Blackrock Neuroport** | 96 | 30 kHz | ‚úÖ Complete | Utah array grid layout, NEV support |
| **Blackrock Cerebus** | 128 | 30 kHz | ‚úÖ Complete | Dual Utah arrays, cortical regions |
| **Intan RHD2132** | 32 | 20 kHz | ‚úÖ Complete | LFP recording, headstage tracking |
| **Intan RHD2164** | 64 | 20 kHz | ‚úÖ Complete | Multi-area recording |
| **Intan RHS128** | 128 | 30 kHz | ‚úÖ Complete | Stimulation-capable |
| **Generic HDF5** | Variable | Variable | ‚úÖ Complete | Auto-detection, flexible loading |
| **Custom Devices** | Any | Any | ‚úÖ Supported | YAML/JSON mapping files |

**Additional Systems** (via configuration):
- **Emotiv EPOC** (14 channels, 128 Hz) - Consumer EEG headsets
- **BioSemi ActiveTwo** (64 channels, 2048 Hz) - High-density research EEG
- **EGI GSN HydroCel** (128 channels, 1000 Hz) - Geodesic Sensor Net
- **Delsys Trigno** (16 channels, 2000 Hz) - Wireless EMG systems
- **Neuropixels** (384 channels, 30 kHz) - High-density neural probes

**üìä Advanced Adapter Features**

- ‚úÖ **Real-time streaming** with <1ms latency
- ‚úÖ **Multi-device pipelines** for simultaneous recording from different systems
- ‚úÖ **Hierarchical compression** (lossless for high-priority, lossy for others)
- ‚úÖ **Channel grouping** by cortical regions, grid rows, or functional areas
- ‚úÖ **High-performance resampling** (250 Hz ‚Üî 30 kHz with FFT/polyphase filters)
- ‚úÖ **Automatic calibration** with gain/offset correction
- ‚úÖ **Partial data loading** from large HDF5 files (memory-efficient)
- ‚úÖ **Device-specific metadata** (Utah array layouts, headstage types, etc.)

**üîÑ Quick Adapter Usage**

```python
# OpenBCI (Scalp EEG)
from bci_compression.adapters.openbci import OpenBCIAdapter
adapter = OpenBCIAdapter(device='cyton_8ch')
processed = adapter.convert(raw_data)

# Blackrock (Intracortical)
from bci_compression.adapters.blackrock import BlackrockAdapter
adapter = BlackrockAdapter(device='neuroport_96ch')
downsampled = adapter.resample_to(raw_data, target_rate=1000)

# Multi-device pipeline
from examples.multi_device_pipeline_example import MultiDevicePipeline
pipeline = MultiDevicePipeline()
pipeline.add_device('openbci', openbci_adapter, priority='normal')
pipeline.add_device('blackrock', blackrock_adapter, priority='high')
compressed = pipeline.process_batch({'openbci': eeg_data, 'blackrock': spike_data})
```

**üìà Performance Benchmarks**

Real measurements from `scripts/profile_adapters.py`:

- **OpenBCI**: 0.059ms full pipeline (170k samples/sec)
- **Blackrock**: 4.216ms full pipeline (7k samples/sec)
- **Intan**: 1.803ms full pipeline (11k samples/sec)
- **Streaming**: <0.1ms average latency

See [Adapters Guide](docs/adapters_guide.md) and [Implementation Summary](docs/adapters_implementation_summary.md) for complete documentation.

### üß† Neural Data Compression Algorithms

#### Lossless Compression - Perfect Signal Preservation

**üöÄ LZ4 - Ultra-Fast Real-Time Compression**

- **What it is**: Industry-standard lossless compression optimized for speed over ratio
- **Why chosen**: Provides >675 MB/s compression, <0.1ms latency for real-time BCI control
- **Neural application**: Ideal for closed-loop prosthetic control where timing is critical
- **Technical specs**: 1.5-2x compression ratio, 3850 MB/s decompression speed
- **Use case**: Motor cortex signals for robotic arm control, real-time feedback systems

**‚ö° Zstandard (ZSTD) - Intelligent Dictionary Compression**

- **What it is**: Facebook's modern compression algorithm with machine learning dictionary training
- **Why chosen**: Adaptive compression models learn from neural data patterns over time
- **Neural application**: Optimizes compression ratios for repetitive neural firing patterns
- **Technical specs**: 2-4x compression ratio, 510 MB/s compression, 1550 MB/s decompression
- **Use case**: Long-term neural recordings, session-based BCI training data

**üî¢ Blosc - Multi-Dimensional Array Specialist**

- **What it is**: High-performance compressor designed specifically for numerical arrays
- **Why chosen**: Leverages SIMD instructions and multi-threading for neural array data
- **Neural application**: Optimized for multi-channel electrode arrays (64-256+ channels)
- **Technical specs**: Blocking technique reduces memory bandwidth, AVX512/NEON acceleration
- **Use case**: High-density neural arrays, spatial correlation across electrode grids

**üß† Neural LZ77 - BCI-Optimized Temporal Compression**

- **What it is**: Custom LZ77 implementation trained on neural signal characteristics
- **Why chosen**: Exploits temporal correlations unique to neural firing patterns
- **Neural application**: Recognizes spike trains, bursting patterns, oscillatory activity
- **Technical specs**: 1.5-3x compression ratio, <1ms latency, 95%+ pattern accuracy
- **Use case**: Single-unit recordings, spike train analysis, temporal pattern preservation

#### Lossy Compression - Quality-Controlled Neural Encoding

**üéµ Perceptual Quantization - Neural Feature Preservation**

- **What it is**: Psychoacoustic principles applied to neural signal frequency domains
- **Why chosen**: Preserves critical neural features while discarding perceptually irrelevant data
- **Neural application**: Maintains action potential shapes, preserves frequency bands (alpha, beta, gamma)
- **Technical specs**: 2-10x compression, 15-25 dB SNR, configurable quality levels
- **Use case**: EEG analysis, spectral power studies, frequency-domain BCI features

**üåä Adaptive Wavelets - Multi-Resolution Neural Analysis**

- **What it is**: Wavelet transforms with neural-specific basis functions and smart thresholding
- **Why chosen**: Natural fit for neural signals with multi-scale temporal dynamics
- **Neural application**: Preserves both fast spikes and slow oscillations simultaneously
- **Technical specs**: 3-15x compression, configurable frequency band preservation
- **Use case**: Multi-scale neural analysis, time-frequency BCI features, neural oscillations

**ü§ñ Deep Autoencoders - Learned Neural Representations**

- **What it is**: Neural networks trained to compress neural data into learned latent spaces
- **Why chosen**: Discovers optimal representations specific to individual neural patterns
- **Neural application**: Personalized compression models adapt to each user's neural signatures
- **Technical specs**: 2-4x compression, learned from user's historical neural data
- **Use case**: Personalized BCIs, adaptive neural interfaces, long-term implant optimization

**üîÆ Transformer Models - Attention-Based Temporal Patterns**

- **What it is**: Multi-head attention mechanisms for compressing temporal neural sequences
- **Why chosen**: Captures long-range dependencies in neural activity patterns
- **Neural application**: Models complex temporal relationships across brain regions
- **Technical specs**: 3-5x compression, 25-35 dB SNR, handles variable-length sequences
- **Use case**: Multi-region neural recordings, cognitive state decoding, complex BCI tasks

**üìä Variational Autoencoders (VAE) - Probabilistic Quality Control**

- **What it is**: Probabilistic encoders with uncertainty quantification for neural data
- **Why chosen**: Provides quality estimates and confidence intervals for compressed neural signals
- **Neural application**: Maintains uncertainty bounds critical for medical-grade BCI applications
- **Technical specs**: Quality-controlled compression with statistical guarantees
- **Use case**: Medical BCIs, safety-critical applications, neural signal validation

#### Advanced Techniques

- **Predictive Coding**: Linear and adaptive prediction models for temporal patterns
- **Context-Aware**: Brain state adaptive compression with real-time switching
- **Multi-Channel**: Spatial correlation exploitation across electrode arrays
- **Spike Detection**: Specialized compression for neural action potentials (>95% accuracy)

### üöÄ Performance Features

**‚ö° Real-Time Processing Guarantees**

- **Ultra-low latency**: < 1ms for basic algorithms, < 2ms for advanced neural methods
- **Deterministic timing**: Hard real-time guarantees for closed-loop BCI systems
- **Streaming architecture**: Bounded memory usage for continuous data processing
- **Pipeline optimization**: Multi-stage processing with minimal buffering delays

**üñ•Ô∏è Hardware Acceleration**

- **GPU acceleration**: CUDA-optimized kernels with CPU fallback (3-5x speedup)
- **SIMD optimization**: AVX512, NEON, and ALTIVEC instruction utilization
- **Multi-threading**: Efficient parallel processing across CPU cores
- **Memory optimization**: Cache-friendly algorithms reduce memory bandwidth

**üì± Mobile & Embedded Support**

- **Power efficiency**: Battery-optimized algorithms for wearable BCI devices
- **Resource constraints**: Minimal memory footprint for embedded systems
- **Cross-platform**: ARM, x86, and RISC-V architecture support
- **Edge computing**: Local processing without cloud dependencies

## üî¨ Compression Technologies Deep Dive

### üè≠ Standard Compression Libraries

**LZ4 - The Speed Champion**

```mermaid
graph LR
    subgraph "LZ4 Pipeline"
        style L1 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style L2 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style L3 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

        L1[Hash Table<br/>Lookup]
        L2[Match<br/>Finding]
        L3[Token<br/>Encoding]
    end

    L1 --> L2 --> L3
```

- **Lightning-fast lossless compression**: Optimized for streaming neural data
- **Minimal CPU overhead**: Perfect for real-time BCI applications
- **Industry standard**: Used by Facebook, Netflix, Linux kernel

**Zstandard (ZSTD) - The Smart Compressor**

```mermaid
graph TB
    subgraph "ZSTD Intelligence"
        style Z1 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style Z2 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style Z3 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff

        Z1[Dictionary<br/>Learning]
        Z2[Entropy<br/>Modeling]
        Z3[Adaptive<br/>Algorithms]
    end

    Z1 --> Z2 --> Z3
```

- **Modern compression**: Facebook's algorithm with dictionary learning for high ratios
- **Neural pattern adaptation**: Learns from repetitive neural firing patterns
- **Scalable performance**: 1-22 compression levels for speed/ratio trade-offs

**Blosc - The Array Specialist**

```mermaid
graph LR
    subgraph "Blosc Architecture"
        style B1 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style B2 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style B3 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

        B1[Shuffle<br/>Filter]
        B2[SIMD<br/>Acceleration]
        B3[Multi-thread<br/>Processing]
    end

    B1 --> B2 --> B3
```

- **Multi-threaded compression library**: Optimized for numerical arrays
- **SIMD optimization**: AVX512, NEON acceleration for neural array data
- **Cache-friendly**: Blocking technique reduces memory bandwidth

### üß† Neural-Specific Algorithms

**Neural LZ77 - BCI-Optimized Pattern Recognition**

- **Custom LZ77 implementation**: Trained on neural signal temporal patterns
- **Spike pattern recognition**: Optimized for action potential sequences
- **Temporal correlation exploitation**: Understands neural firing rhythms

**Perceptual Quantization - Frequency-Domain Intelligence**

- **Psychoacoustically-inspired**: Adapted from audio compression for neural frequencies
- **Critical band preservation**: Maintains alpha, beta, gamma frequency information
- **Configurable quality**: Adjustable SNR levels from 15-35 dB

**Adaptive Wavelets - Multi-Scale Neural Analysis**

- **Multi-resolution analysis**: Preserves both fast spikes and slow oscillations
- **Neural-specific basis functions**: Optimized for biological signal characteristics
- **Smart thresholding**: Preserves critical neural features while removing noise

### ü§ñ AI/ML Compression Revolution

**Deep Autoencoders - Learned Neural Representations**

- **Personalized compression**: Models adapt to individual neural signatures
- **Latent space optimization**: Discovers optimal representations for neural data
- **Transfer learning**: Pre-trained models adapt to new subjects quickly

**Variational Autoencoders (VAE) - Probabilistic Intelligence**

- **Uncertainty quantification**: Provides confidence intervals for compressed data
- **Quality-controlled compression**: Statistical guarantees for medical applications
- **Generative modeling**: Can synthesize realistic neural data for training

**Transformer Models - Attention-Based Neural Compression**

- **Multi-head attention**: Captures long-range dependencies in neural sequences
- **Sequence-to-sequence**: Handles variable-length neural recordings
- **State-of-the-art performance**: 25-35 dB SNR with 3-5x compression

**Predictive Coding - Temporal Pattern Prediction**

- **Linear/nonlinear prediction**: Models temporal dependencies in neural signals
- **Adaptive algorithms**: Continuously update models based on signal characteristics
- **Real-time learning**: Updates compression models during acquisition

### üìä Technical Specifications & Performance Matrix

#### Core Algorithm Performance

| Algorithm | Compression Ratio | Latency | Throughput | Quality | Memory Usage | GPU Speedup |
|-----------|------------------|---------|------------|---------|--------------|-------------|
| **LZ4** | 1.5-2x | < 0.1ms | 675+ MB/s | Lossless | 32KB | 2x |
| **Zstandard** | 2-4x | < 0.5ms | 510 MB/s | Lossless | 128KB | 3x |
| **Blosc** | 1.8-3x | < 0.2ms | 800+ MB/s | Lossless | 64KB | 4x |
| **Neural LZ77** | 1.5-3x | < 1ms | 400 MB/s | Lossless | 256KB | 2.5x |
| **Perceptual Quant** | 2-10x | < 1ms | 300 MB/s | 15-25 dB SNR | 512KB | 5x |
| **Adaptive Wavelets** | 3-15x | < 1ms | 250 MB/s | Configurable | 1MB | 6x |
| **Transformers** | 3-5x | < 2ms | 150 MB/s | 25-35 dB SNR | 2MB | 8x |
| **VAE** | 2-8x | < 5ms | 100 MB/s | Statistical | 4MB | 10x |

#### Neural Signal Specific Performance

| Signal Type | Best Algorithm | Compression Ratio | Latency | Fidelity |
|-------------|---------------|------------------|---------|----------|
| **Motor Cortex** | LZ4 + Neural LZ77 | 2.1x | < 0.5ms | 100% |
| **Visual Cortex** | Zstandard | 3.2x | < 0.8ms | 100% |
| **EMG Signals** | Blosc + Wavelets | 8.5x | < 1.2ms | 98.5% |
| **EEG Arrays** | Perceptual Quant | 6.8x | < 1.5ms | 22 dB SNR |
| **Spike Trains** | Neural LZ77 | 2.8x | < 0.3ms | 99.8% |
| **Multi-Channel** | Blosc | 4.1x | < 0.4ms | 100% |

#### Hardware Platform Support

| Platform | CPU Architecture | GPU Support | Max Channels | Max Sampling Rate |
|----------|-----------------|-------------|--------------|------------------|
| **Desktop** | x86-64, ARM64 | CUDA, OpenCL | 1024+ | 50kHz |
| **Mobile** | ARM Cortex-A | GPU Compute | 256 | 30kHz |
| **Embedded** | ARM Cortex-M | None | 64 | 10kHz |
| **FPGA** | Custom | Hardware | 2048+ | 100kHz |
| **Cloud** | x86-64 | CUDA, TPU | Unlimited | Unlimited |

### üéØ Specialized Applications & Use Cases

#### Medical-Grade BCI Applications

```mermaid
graph TB
    subgraph "üè• Clinical BCIs"
        style C1 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style C2 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style C3 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

        C1[Epilepsy<br/>Monitoring]
        C2[Deep Brain<br/>Stimulation]
        C3[Neural<br/>Prosthetics]
    end

    subgraph "‚ö° Real-Time Requirements"
        style R1 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style R2 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style R3 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff

        R1[< 1ms Latency<br/>LZ4 + Neural LZ77]
        R2[< 500Œºs Latency<br/>LZ4 Only]
        R3[< 2ms Latency<br/>Advanced ML]
    end

    subgraph "üìä Data Characteristics"
        style D1 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style D2 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style D3 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

        D1[128+ Channels<br/>30kHz Sampling]
        D2[32 Channels<br/>10kHz Sampling]
        D3[256+ Channels<br/>40kHz Sampling]
    end

    C1 --> R3
    C2 --> R2
    C3 --> R1

    R1 --> D3
    R2 --> D2
    R3 --> D1
```

#### Performance vs Quality Trade-offs

```mermaid
graph LR
    subgraph "üèÉ Ultra-Fast (< 0.1ms)"
        style UF1 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style UF2 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

        UF1[LZ4<br/>1.5-2x ratio]
        UF2[Blosc<br/>1.8-3x ratio]
    end

    subgraph "‚ö° Fast (< 1ms)"
        style F1 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style F2 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style F3 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff

        F1[Zstandard<br/>2-4x ratio]
        F2[Neural LZ77<br/>1.5-3x ratio]
        F3[Perceptual Quant<br/>2-10x ratio]
    end

    subgraph "üß† Advanced (< 2ms)"
        style A1 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style A2 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

        A1[Transformers<br/>3-5x ratio]
        A2[VAE<br/>2-8x ratio]
    end

    UF1 --> F1
    UF2 --> F2
    F1 --> A1
    F2 --> A2
    F3 --> A1
```

#### Specialized Signal Support

**üß† EMG Compression**

- **Specialized algorithms**: Electromyography signals (5-12x compression)
- **Muscle artifact handling**: Optimized for movement-related noise
- **Real-time feedback**: < 500Œºs latency for prosthetic control

**üì° Multi-Channel Arrays**

- **Spatial correlation**: High-density electrode grids (256+ channels)
- **Blosc optimization**: Columnar compression for array data
- **Scalable architecture**: Supports up to 2048 channels simultaneously

**üì± Mobile/Embedded BCIs**

- **Power efficiency**: Battery-optimized algorithms for wearable devices
- **Resource constraints**: Minimal memory footprint (< 1MB)
- **ARM optimization**: NEON SIMD instruction utilization

**‚òÅÔ∏è Cloud Analytics**

- **Batch processing**: High-ratio compression for long-term storage
- **Dictionary training**: Zstandard with learned neural patterns
- **Scalable processing**: Distributed compression across multiple GPUs

## üì° API Documentation

### Core Compression API

```python
from neural_compression import NeuralCompressor, CompressionConfig

# Initialize compressor with GPU acceleration
compressor = NeuralCompressor(
    algorithm='neural_lz77',
    gpu_enabled=True,
    real_time=True
)

# Compress neural data stream
compressed_data = compressor.compress(
    neural_signals,  # numpy array (channels, samples)
    quality_level=0.95,  # 0.0-1.0 for lossy algorithms
    preserve_spikes=True  # maintain action potential fidelity
)

# Real-time streaming compression
stream = compressor.create_stream(
    buffer_size=1024,
    overlap=128,
    latency_target=0.5  # milliseconds
)

for chunk in neural_data_stream:
    compressed_chunk = stream.process(chunk)
    # < 1ms processing time guaranteed
```

### Algorithm Selection API

```python
from neural_compression import AlgorithmSelector

# Automatic algorithm selection based on signal characteristics
selector = AlgorithmSelector()
optimal_config = selector.analyze_and_recommend(
    signal_data=neural_array,
    sampling_rate=30000,  # Hz
    channel_count=256,
    latency_requirement=1.0,  # ms
    quality_requirement=0.98  # fidelity score
)

# Returns optimized configuration
# optimal_config.algorithm -> 'blosc' for multi-channel
# optimal_config.parameters -> {compression_level: 5, threads: 4}
```

### Performance Monitoring API

```python
from neural_compression import PerformanceMonitor

monitor = PerformanceMonitor()

# Real-time performance tracking
with monitor.track_compression() as tracker:
    result = compressor.compress(data)

    # Automatic metrics collection
    metrics = tracker.get_metrics()
    # metrics.latency -> 0.8ms
    # metrics.throughput -> 450 MB/s
    # metrics.compression_ratio -> 2.3x
    # metrics.fidelity_score -> 0.987
```

### WebSocket Streaming API

```python
import asyncio
from neural_compression.streaming import NeuralWebSocket

async def stream_neural_data():
    websocket = NeuralWebSocket(
        host='localhost',
        port=8080,
        compression='lz4',
        real_time=True
    )

    async for compressed_chunk in websocket.stream():
        # Receive compressed neural data
        decompressed = websocket.decompress(compressed_chunk)
        # Process in real-time (< 1ms latency)
```

### REST API Endpoints

**Compression Service** - `POST /api/v1/compress`

```json
{
  "data": "base64_encoded_neural_data",
  "algorithm": "neural_lz77",
  "config": {
    "quality": 0.95,
    "gpu_acceleration": true,
    "real_time": true
  }
}
```

**Algorithm Recommendation** - `POST /api/v1/recommend`

```json
{
  "signal_characteristics": {
    "sampling_rate": 30000,
    "channel_count": 128,
    "signal_type": "motor_cortex",
    "noise_level": 0.05
  },
  "requirements": {
    "max_latency_ms": 1.0,
    "min_fidelity": 0.98,
    "target_compression": 3.0
  }
}
```

**Performance Metrics** - `GET /api/v1/metrics`

```json
{
  "current_throughput": "675 MB/s",
  "average_latency": "0.45ms",
  "compression_ratio": "2.8x",
  "gpu_utilization": "23%",
  "active_streams": 12
}
```

### Configuration Management

```python
from neural_compression import CompressionConfig

# Algorithm-specific configurations
configs = {
    'real_time_control': CompressionConfig(
        algorithm='lz4',
        latency_target=0.1,  # 100Œºs for prosthetic control
        quality=1.0,  # lossless
        gpu_enabled=False  # CPU for deterministic timing
    ),

    'high_density_arrays': CompressionConfig(
        algorithm='blosc',
        threads=8,
        compression_level=6,
        shuffle=True,  # optimize for array patterns
        gpu_enabled=True
    ),

    'analysis_storage': CompressionConfig(
        algorithm='zstd',
        compression_level=19,  # maximum ratio
        dictionary_training=True,
        quality=1.0  # lossless for analysis
    ),

    'mobile_streaming': CompressionConfig(
        algorithm='perceptual_quantization',
        quality=0.85,  # balanced quality/size
        power_efficient=True,
        memory_limit='256MB'
    )
}
```

## üèÉ‚Äç‚ôÇÔ∏è Getting Started

1. **Clone the repository**

   ```bash
   git clone https://github.com/hkevin01/brain-computer-compression.git
   cd brain-computer-compression
   ```

2. **Start with Docker** (recommended)

   ```bash
   ./run.sh up
   ```

3. **Or manual setup**

   ```bash
   ./scripts/setup/setup.sh
   ```

4. **Access the dashboard**
   - Open <http://localhost:3000> in your browser
   - Or run `./run.sh gui:open`

5. **API access**
   - REST API: <http://localhost:8000/docs>
   - WebSocket: `ws://localhost:8080/stream`
   - Metrics: <http://localhost:8000/metrics>

## üß™ Benchmarking & Testing

### Performance Benchmarks

**Real-Time Processing Benchmarks**

```mermaid
graph LR
    subgraph "‚ö° Latency Benchmarks (ms)"
        style L1 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style L2 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style L3 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff
        style L4 fill:#1a202c,stroke:#2d3748,stroke-width:2px,color:#ffffff

        L1[LZ4<br/>0.08ms]
        L2[Blosc<br/>0.15ms]
        L3[ZSTD<br/>0.42ms]
        L4[Neural LZ77<br/>0.85ms]
    end

    subgraph "üöÄ Throughput (MB/s)"
        style T1 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style T2 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style T3 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff
        style T4 fill:#2d3748,stroke:#4a5568,stroke-width:2px,color:#ffffff

        T1[LZ4<br/>675 MB/s]
        T2[Blosc<br/>820 MB/s]
        T3[ZSTD<br/>510 MB/s]
        T4[Neural LZ77<br/>385 MB/s]
    end

    L1 --> T1
    L2 --> T2
    L3 --> T3
    L4 --> T4
```

**Neural Data Specific Benchmarks**

| Dataset | Algorithm | Compression Ratio | Latency | SNR | Spike Accuracy |
|---------|-----------|------------------|---------|-----|----------------|
| **Motor Cortex (128ch, 30kHz)** | LZ4 + Neural LZ77 | 2.1x | 0.5ms | ‚àû (lossless) | 100% |
| **Visual Cortex (256ch, 40kHz)** | Blosc + ZSTD | 3.8x | 0.8ms | ‚àû (lossless) | 100% |
| **EMG Arrays (64ch, 10kHz)** | Perceptual Quant | 8.2x | 1.2ms | 28.5 dB | 98.7% |
| **EEG (32ch, 1kHz)** | Adaptive Wavelets | 12.5x | 1.8ms | 32.1 dB | 99.2% |
| **Spike Trains (Single Unit)** | Neural LZ77 | 2.9x | 0.3ms | ‚àû (lossless) | 99.9% |

### Test Suite Coverage

**Unit Tests** - Core Algorithm Validation

```bash
# Run all compression algorithm tests
pytest tests/algorithms/ -v --cov=neural_compression

# Test specific algorithms
pytest tests/algorithms/test_lz4_compression.py
pytest tests/algorithms/test_neural_lz77.py
pytest tests/algorithms/test_gpu_acceleration.py

# Performance regression tests
pytest tests/performance/ --benchmark-only
```

**Integration Tests** - End-to-End Validation

```bash
# Full pipeline tests with real neural data
pytest tests/integration/test_neural_pipeline.py

# Real-time streaming tests
pytest tests/integration/test_realtime_processing.py

# GPU acceleration integration
pytest tests/integration/test_gpu_pipeline.py
```

**Benchmark Tests** - Performance Validation

```bash
# Comprehensive benchmarking suite
python scripts/benchmark/run_benchmarks.py

# Specific performance tests
python scripts/benchmark/latency_benchmark.py
python scripts/benchmark/throughput_benchmark.py
python scripts/benchmark/compression_ratio_benchmark.py
```

### Test Data Sources

**Synthetic Neural Data**

- **Generated spike trains**: Poisson processes with realistic firing rates
- **Multi-channel arrays**: Simulated electrode grids with spatial correlations
- **Noise models**: Realistic thermal and electronic noise characteristics
- **Artifact simulation**: Movement artifacts, line noise, electrode drift

**Real Neural Datasets**

- **Motor cortex recordings**: Utah array data from macaque experiments
- **Visual cortex data**: Multi-electrode recordings during visual stimulation
- **Human EEG/ECoG**: Clinical datasets with appropriate anonymization
- **EMG recordings**: High-density surface and intramuscular recordings

### Continuous Integration

**GitHub Actions Workflow**

```yaml
name: Neural Compression CI
on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, 3.10, 3.11]

    steps:
    - uses: actions/checkout@v3
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run unit tests
      run: pytest tests/ --cov=neural_compression

    - name: Run integration tests
      run: pytest tests/integration/

    - name: Performance benchmarks
      run: python scripts/benchmark/ci_benchmarks.py

    - name: Upload coverage
      uses: codecov/codecov-action@v3
```

**Performance Regression Detection**

- **Automatic benchmarking**: Every commit tested for performance regressions
- **Latency monitoring**: Alerts if processing latency exceeds thresholds
- **Memory usage tracking**: Detects memory leaks in streaming scenarios
- **GPU utilization monitoring**: Ensures efficient hardware acceleration usage

### Quality Assurance

**Code Quality Tools**

```bash
# Code formatting
black neural_compression/
isort neural_compression/

# Type checking
mypy neural_compression/

# Linting
flake8 neural_compression/
pylint neural_compression/

# Security scanning
bandit -r neural_compression/
```

**Documentation Testing**

```bash
# Docstring examples
python -m doctest neural_compression/*.py

# Documentation build
sphinx-build -b html docs/ docs/_build/

# API documentation validation
python scripts/validate_api_docs.py
```

## ÔøΩ Project Structure

```
brain-computer-compression/
‚îú‚îÄ‚îÄ üì¶ neural_compression/          # Core compression library
‚îÇ   ‚îú‚îÄ‚îÄ üß† algorithms/              # Compression algorithms
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lossless/               # Lossless compression (LZ4, ZSTD, Blosc)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lossy/                  # Lossy compression (wavelets, quantization)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ neural/                 # Neural-specific algorithms (Neural LZ77)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ai_powered/             # AI/ML compression (autoencoders, transformers)
‚îÇ   ‚îú‚îÄ‚îÄ üöÄ gpu/                     # GPU acceleration modules
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cuda_kernels/           # Custom CUDA implementations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cupy_wrappers/          # CuPy integration layer
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ memory_management/      # GPU memory optimization
‚îÇ   ‚îú‚îÄ‚îÄ üìä streaming/               # Real-time processing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ buffers/                # Circular buffers and windowing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines/              # Processing pipelines
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ websockets/             # WebSocket streaming
‚îÇ   ‚îú‚îÄ‚îÄ üîß utils/                   # Utility functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ signal_processing/      # Signal preprocessing
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ performance/            # Performance monitoring
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_formats/           # Neural data format support
‚îÇ   ‚îî‚îÄ‚îÄ üì° api/                     # API interfaces
‚îÇ       ‚îú‚îÄ‚îÄ rest/                   # REST API endpoints
‚îÇ       ‚îú‚îÄ‚îÄ websocket/              # WebSocket handlers
‚îÇ       ‚îî‚îÄ‚îÄ config/                 # Configuration management
‚îú‚îÄ‚îÄ üåê web/                         # Web dashboard
‚îÇ   ‚îú‚îÄ‚îÄ frontend/                   # React/Next.js frontend
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/             # UI components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/                  # Dashboard pages
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hooks/                  # Custom React hooks
‚îÇ   ‚îî‚îÄ‚îÄ backend/                    # FastAPI backend
‚îÇ       ‚îú‚îÄ‚îÄ routers/                # API route handlers
‚îÇ       ‚îú‚îÄ‚îÄ services/               # Business logic
‚îÇ       ‚îî‚îÄ‚îÄ models/                 # Data models
‚îú‚îÄ‚îÄ üß™ tests/                       # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ unit/                       # Unit tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ algorithms/             # Algorithm-specific tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gpu/                    # GPU acceleration tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ streaming/              # Real-time processing tests
‚îÇ   ‚îú‚îÄ‚îÄ integration/                # Integration tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipelines/              # End-to-end pipeline tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api/                    # API integration tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ performance/            # Performance validation
‚îÇ   ‚îî‚îÄ‚îÄ benchmark/                  # Benchmarking suite
‚îÇ       ‚îú‚îÄ‚îÄ latency/                # Latency benchmarks
‚îÇ       ‚îú‚îÄ‚îÄ throughput/             # Throughput benchmarks
‚îÇ       ‚îî‚îÄ‚îÄ compression_ratio/      # Compression ratio tests
‚îú‚îÄ‚îÄ üìñ docs/                        # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ api/                        # API documentation
‚îÇ   ‚îú‚îÄ‚îÄ guides/                     # User guides and tutorials
‚îÇ   ‚îú‚îÄ‚îÄ algorithms/                 # Algorithm documentation
‚îÇ   ‚îú‚îÄ‚îÄ benchmarks/                 # Performance reports
‚îÇ   ‚îî‚îÄ‚îÄ project/                    # Project documentation
‚îú‚îÄ‚îÄ üê≥ docker/                      # Docker configuration
‚îÇ   ‚îú‚îÄ‚îÄ services/                   # Individual service containers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ compression/            # Compression service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ web/                    # Web dashboard
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ gpu/                    # GPU-enabled containers
‚îÇ   ‚îú‚îÄ‚îÄ compose/                    # Docker Compose files
‚îÇ   ‚îî‚îÄ‚îÄ scripts/                    # Container scripts
‚îú‚îÄ‚îÄ üîß scripts/                     # Utility scripts
‚îÇ   ‚îú‚îÄ‚îÄ setup/                      # Environment setup
‚îÇ   ‚îú‚îÄ‚îÄ benchmark/                  # Benchmarking scripts
‚îÇ   ‚îú‚îÄ‚îÄ tools/                      # Development tools
‚îÇ   ‚îî‚îÄ‚îÄ deployment/                 # Deployment scripts
‚îú‚îÄ‚îÄ üìä data/                        # Sample and test data
‚îÇ   ‚îú‚îÄ‚îÄ synthetic/                  # Generated neural data
‚îÇ   ‚îú‚îÄ‚îÄ real/                       # Real neural recordings
‚îÇ   ‚îî‚îÄ‚îÄ benchmarks/                 # Benchmark datasets
‚îî‚îÄ‚îÄ üìã config/                      # Configuration files
    ‚îú‚îÄ‚îÄ algorithms/                 # Algorithm configurations
    ‚îú‚îÄ‚îÄ deployment/                 # Deployment configurations
    ‚îî‚îÄ‚îÄ development/                # Development settings
```

### Core Components Deep Dive

#### üß† Neural Compression Algorithms (`neural_compression/algorithms/`)

**Lossless Compression** (`lossless/`)
- `lz4_compression.py` - Ultra-fast LZ4 implementation with neural optimizations
- `zstd_compression.py` - Zstandard with dictionary learning for neural patterns
- `blosc_compression.py` - Multi-threaded array compression with SIMD acceleration
- `neural_lz77.py` - Custom LZ77 variant trained on neural signal characteristics

**Lossy Compression** (`lossy/`)
- `perceptual_quantization.py` - Psychoacoustic principles adapted for neural frequencies
- `adaptive_wavelets.py` - Multi-resolution wavelet compression with neural-specific basis
- `predictive_coding.py` - Linear and adaptive prediction models for temporal patterns

**AI-Powered Compression** (`ai_powered/`)
- `autoencoders.py` - Deep autoencoder models for learned neural representations
- `transformers.py` - Multi-head attention models for sequence compression
- `vae_compression.py` - Variational autoencoders with uncertainty quantification

#### üöÄ GPU Acceleration (`neural_compression/gpu/`)

**CUDA Kernels** (`cuda_kernels/`)
- `lz4_cuda.cu` - Custom CUDA implementation of LZ4 compression
- `wavelet_cuda.cu` - GPU-accelerated wavelet transforms
- `neural_network_cuda.cu` - Optimized neural network inference kernels

**Memory Management** (`memory_management/`)
- `gpu_buffers.py` - Efficient GPU memory allocation and streaming
- `memory_pool.py` - Memory pool management for continuous processing
- `transfer_optimization.py` - CPU-GPU memory transfer optimization

#### üìä Real-Time Streaming (`neural_compression/streaming/`)

**Buffer Management** (`buffers/`)
- `circular_buffer.py` - Lock-free circular buffers for streaming data
- `sliding_window.py` - Overlapping window processing for continuous signals
- `adaptive_buffer.py` - Dynamic buffer sizing based on processing load

**Processing Pipelines** (`pipelines/`)
- `realtime_pipeline.py` - Real-time processing pipeline with guaranteed latency
- `batch_pipeline.py` - High-throughput batch processing for offline analysis
- `streaming_pipeline.py` - Continuous streaming with backpressure handling

#### üåê Web Dashboard (`web/`)

**Frontend** (`frontend/`)
- `components/CompressionMonitor.tsx` - Real-time compression performance monitoring
- `components/AlgorithmSelector.tsx` - Interactive algorithm selection interface
- `components/PerformanceCharts.tsx` - Real-time performance visualization
- `pages/Dashboard.tsx` - Main dashboard with compression metrics
- `pages/Benchmarks.tsx` - Performance benchmarking interface

**Backend** (`backend/`)
- `routers/compression.py` - Compression API endpoints
- `routers/streaming.py` - WebSocket streaming endpoints
- `services/compression_service.py` - Core compression business logic
- `models/neural_data.py` - Neural data models and validation

### Configuration Management

#### Algorithm Configurations (`config/algorithms/`)

```yaml
# config/algorithms/realtime.yaml
realtime_compression:
  algorithm: "lz4"
  max_latency_ms: 1.0
  gpu_enabled: false  # CPU for deterministic timing
  buffer_size: 1024

# config/algorithms/high_ratio.yaml
high_ratio_compression:
  algorithm: "zstd"
  compression_level: 19
  dictionary_training: true
  gpu_enabled: true

# config/algorithms/neural_optimized.yaml
neural_optimized:
  algorithm: "neural_lz77"
  spike_detection: true
  temporal_correlation: true
  adaptive_learning: true
```

#### Deployment Configurations (`config/deployment/`)

```yaml
# config/deployment/production.yaml
production:
  gpu_memory_limit: "8GB"
  max_concurrent_streams: 100
  monitoring_enabled: true
  logging_level: "INFO"

# config/deployment/development.yaml
development:
  gpu_memory_limit: "2GB"
  max_concurrent_streams: 10
  monitoring_enabled: true
  logging_level: "DEBUG"
  profiling_enabled: true
```

## ü§ù Contributing & Development

### Development Workflow

Development follows a structured process with quality gates:

#### Setting Up Development Environment

##### Option 1: Docker Development (Recommended)

```bash
# Clone and start development environment
git clone https://github.com/hkevin01/brain-computer-compression.git
cd brain-computer-compression

# Start development services with hot-reload
./run.sh dev

# Access development tools
# - Code: http://localhost:8080 (VS Code in browser)
# - API: http://localhost:8000/docs
# - Dashboard: http://localhost:3000
# - Jupyter: http://localhost:8888
```

##### Option 2: Local Development

```bash
# Python environment setup
python3.10 -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-dev.txt

# Install project in development mode
pip install -e .

# Install pre-commit hooks
pre-commit install
```

#### Code Standards & Guidelines

##### Python Code Style

Follow PEP 8 with these specific guidelines:

```python
# Type hints are required for all public functions
def compress_neural_data(
    data: np.ndarray,
    algorithm: str = "lz4",
    quality: float = 1.0,
    gpu_enabled: bool = False
) -> CompressionResult:
    """
    Compress neural data using specified algorithm.

    Args:
        data: Neural signal array (channels, samples)
        algorithm: Compression algorithm name
        quality: Quality level (0.0-1.0)
        gpu_enabled: Enable GPU acceleration

    Returns:
        Compression result with metrics

    Raises:
        ValueError: If algorithm not supported
        MemoryError: If insufficient GPU memory
    """
    pass
```

##### Performance Requirements

All contributions must meet these performance benchmarks:

- **Latency**: < 1ms for real-time algorithms, < 5ms for advanced algorithms
- **Throughput**: Minimum 100 MB/s compression speed
- **Memory**: Bounded memory usage for streaming scenarios
- **GPU Efficiency**: > 70% GPU utilization when GPU-enabled

##### Testing Requirements

Unit Tests (Required for all PRs):

```bash
# Run specific test categories
pytest tests/unit/algorithms/ -v --cov=neural_compression.algorithms
pytest tests/unit/gpu/ -v --cov=neural_compression.gpu
pytest tests/unit/streaming/ -v --cov=neural_compression.streaming

# Minimum coverage: 85% for new code
pytest --cov=neural_compression --cov-report=html --cov-fail-under=85
```

#### Pull Request Requirements

Checklist for all PRs:

- [ ] All tests pass (`pytest tests/`)
- [ ] Performance benchmarks meet requirements
- [ ] Code coverage ‚â• 85% for new code
- [ ] Documentation updated (API docs, README if needed)
- [ ] Type hints added for all public functions
- [ ] Docstrings follow Google/NumPy style
- [ ] No performance regressions detected
- [ ] GPU compatibility verified (if applicable)

Performance Validation:

```bash
# Before submitting PR, run full validation
./scripts/validate_pr.sh

# This script runs:
# - All unit and integration tests
# - Performance regression testing
# - Code quality checks (flake8, mypy, black)
# - Documentation validation
# - Security scanning (bandit)
```

#### Getting Help

##### Development Support

- **GitHub Discussions**: For design questions and general development help
- **Slack Channel**: `#neural-compression-dev` for real-time collaboration
- **Weekly Office Hours**: Thursdays 2-3 PM EST for direct developer support
- **Documentation**: [docs/development/](docs/development/) for detailed guides

##### Issue Reporting

- **Bug Reports**: Use GitHub Issues with the `bug` label
- **Feature Requests**: Use GitHub Issues with the `enhancement` label
- **Performance Issues**: Include benchmark results and system specifications
- **GPU Issues**: Provide CUDA version, driver version, and hardware details

## üìñ Learn More

- **API Documentation**: <http://localhost:8000/docs> (when running)
- **Project Guides**: [docs/guides/](docs/guides/)
- **Development Setup**: [docs/CONTRIBUTING.md](docs/CONTRIBUTING.md)
- **Architecture Overview**: [docs/project/](docs/project/)

## üß† Memory Bank

This project maintains a comprehensive memory bank for tracking decisions, implementations, and changes:

- **üìù App Description**: [memory-bank/app-description.md](memory-bank/app-description.md) - Comprehensive project overview and mission
- **üìã Implementation Plans**: [memory-bank/implementation-plans/](memory-bank/implementation-plans/) - ACID-structured feature development plans
- **üèóÔ∏è Architecture Decisions**: [memory-bank/architecture-decisions/](memory-bank/architecture-decisions/) - ADRs documenting key technical decisions
- **üìä Change Log**: [memory-bank/change-log.md](memory-bank/change-log.md) - Complete history of project modifications

The memory bank follows ACID principles (Atomic, Consistent, Isolated, Durable) to ensure clear, traceable, and maintainable documentation of all project evolution.

## üìÑ License

MIT License - see [LICENSE](LICENSE) for details.

---

**üéØ Goal**: Efficient neural data compression for next-generation brain-computer interfaces.
