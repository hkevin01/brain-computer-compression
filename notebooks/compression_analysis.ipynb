{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e9340c2",
   "metadata": {},
   "source": [
    "# Brain-Computer Interface Data Compression Analysis\n",
    "\n",
    "This notebook provides a comprehensive analysis of different compression algorithms for neural data streams. We'll compare various compression techniques including lossless, lossy, and deep learning-based approaches to understand their trade-offs in terms of compression ratio, processing speed, and signal quality preservation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The analysis covers:\n",
    "- **Lossless Compression**: Adaptive LZ, Dictionary-based compression\n",
    "- **Lossy Compression**: Quantization, Wavelet transform\n",
    "- **Deep Learning**: Autoencoder-based compression\n",
    "- **Performance Metrics**: Compression ratio, latency, SNR preservation\n",
    "- **Neural Data Characteristics**: Multi-channel recordings, temporal correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a85a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import signal\n",
    "from scipy.fft import fft, fftfreq\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# BCI Compression toolkit imports\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from bci_compression import NeuralCompressor, load_neural_data\n",
    "from bci_compression.data_processing import (\n",
    "    generate_synthetic_neural_data, \n",
    "    apply_bandpass_filter,\n",
    "    apply_notch_filter\n",
    ")\n",
    "from bci_compression.algorithms import (\n",
    "    AdaptiveLZCompressor,\n",
    "    QuantizationCompressor, \n",
    "    WaveletCompressor,\n",
    "    AutoencoderCompressor\n",
    ")\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af05ddfd",
   "metadata": {},
   "source": [
    "## Load and Explore Neural Dataset\n",
    "\n",
    "We'll generate synthetic neural data that mimics real brain-computer interface recordings. The synthetic data includes:\n",
    "- Multiple neural channels (64 channels typical for BCI)\n",
    "- Realistic neural oscillations (alpha, beta, gamma bands)\n",
    "- Spike activity with physiological characteristics\n",
    "- Background noise and cross-channel correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8670c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic neural data\n",
    "n_channels = 64\n",
    "n_samples = 30000  # 30 seconds at 1 kHz\n",
    "fs = 1000.0  # Sampling frequency in Hz\n",
    "\n",
    "print(\"Generating synthetic neural data...\")\n",
    "neural_data, metadata = generate_synthetic_neural_data(\n",
    "    n_channels=n_channels,\n",
    "    n_samples=n_samples,\n",
    "    fs=fs,\n",
    "    noise_level=0.1,\n",
    "    spike_rate=5.0,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"Data shape: {neural_data.shape}\")\n",
    "print(f\"Data type: {neural_data.dtype}\")\n",
    "print(f\"Data size: {neural_data.nbytes / 1e6:.2f} MB\")\n",
    "print(f\"Duration: {metadata['duration']:.1f} seconds\")\n",
    "print(f\"Sampling rate: {metadata['fs']:.0f} Hz\")\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"\\nData Statistics:\")\n",
    "print(f\"Mean: {neural_data.mean():.4f}\")\n",
    "print(f\"Std: {neural_data.std():.4f}\")\n",
    "print(f\"Min: {neural_data.min():.4f}\")\n",
    "print(f\"Max: {neural_data.max():.4f}\")\n",
    "\n",
    "# Create a DataFrame for easier analysis\n",
    "time_vector = np.arange(n_samples) / fs\n",
    "df_metadata = pd.DataFrame([metadata])\n",
    "print(f\"\\nMetadata:\")\n",
    "print(df_metadata.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d1bc54",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Neural data often requires preprocessing to remove artifacts and enhance signal quality. We'll apply:\n",
    "- Bandpass filtering to isolate neural frequencies of interest\n",
    "- Notch filtering to remove power line interference\n",
    "- Normalization to standardize signal amplitudes across channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0b33ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to neural data\n",
    "print(\"Applying preprocessing...\")\n",
    "\n",
    "# 1. Bandpass filter (1-100 Hz) to remove DC drift and high-frequency noise\n",
    "filtered_data = apply_bandpass_filter(\n",
    "    neural_data, \n",
    "    lowcut=1.0, \n",
    "    highcut=100.0, \n",
    "    fs=fs, \n",
    "    order=4\n",
    ")\n",
    "\n",
    "# 2. Notch filter to remove 50 Hz power line interference\n",
    "filtered_data = apply_notch_filter(\n",
    "    filtered_data,\n",
    "    notch_freq=50.0,\n",
    "    fs=fs,\n",
    "    quality_factor=30.0\n",
    ")\n",
    "\n",
    "# 3. Z-score normalization per channel\n",
    "preprocessed_data = np.zeros_like(filtered_data)\n",
    "for ch in range(n_channels):\n",
    "    channel_data = filtered_data[ch, :]\n",
    "    preprocessed_data[ch, :] = (channel_data - channel_data.mean()) / channel_data.std()\n",
    "\n",
    "print(f\"Original data range: [{neural_data.min():.3f}, {neural_data.max():.3f}]\")\n",
    "print(f\"Preprocessed data range: [{preprocessed_data.min():.3f}, {preprocessed_data.max():.3f}]\")\n",
    "\n",
    "# Visualize the effect of preprocessing\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot sample channel before and after preprocessing\n",
    "ch_idx = 0\n",
    "time_slice = slice(0, 5000)  # First 5 seconds\n",
    "\n",
    "axes[0, 0].plot(time_vector[time_slice], neural_data[ch_idx, time_slice])\n",
    "axes[0, 0].set_title('Original Signal (Channel 1)')\n",
    "axes[0, 0].set_xlabel('Time (s)')\n",
    "axes[0, 0].set_ylabel('Amplitude')\n",
    "\n",
    "axes[0, 1].plot(time_vector[time_slice], preprocessed_data[ch_idx, time_slice])\n",
    "axes[0, 1].set_title('Preprocessed Signal (Channel 1)')\n",
    "axes[0, 1].set_xlabel('Time (s)')\n",
    "axes[0, 1].set_ylabel('Normalized Amplitude')\n",
    "\n",
    "# Power spectral density comparison\n",
    "freqs = fftfreq(n_samples, 1/fs)[:n_samples//2]\n",
    "original_psd = np.abs(fft(neural_data[ch_idx, :])[:n_samples//2])**2\n",
    "preprocessed_psd = np.abs(fft(preprocessed_data[ch_idx, :])[:n_samples//2])**2\n",
    "\n",
    "axes[1, 0].semilogy(freqs, original_psd)\n",
    "axes[1, 0].set_title('Original PSD')\n",
    "axes[1, 0].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 0].set_ylabel('Power')\n",
    "axes[1, 0].set_xlim(0, 150)\n",
    "\n",
    "axes[1, 1].semilogy(freqs, preprocessed_psd)\n",
    "axes[1, 1].set_title('Preprocessed PSD')\n",
    "axes[1, 1].set_xlabel('Frequency (Hz)')\n",
    "axes[1, 1].set_ylabel('Power')\n",
    "axes[1, 1].set_xlim(0, 150)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c46ac",
   "metadata": {},
   "source": [
    "## Compression Algorithm Setup\n",
    "\n",
    "We'll test different compression algorithms optimized for neural data characteristics:\n",
    "\n",
    "1. **Lossless Algorithms**: Preserve exact signal reconstruction\n",
    "   - Adaptive LZ compression\n",
    "   - Dictionary-based compression\n",
    "\n",
    "2. **Lossy Algorithms**: Allow controlled quality loss for higher compression\n",
    "   - Quantization-based compression\n",
    "   - Wavelet transform compression\n",
    "\n",
    "3. **Deep Learning**: Neural network-based compression\n",
    "   - Autoencoder compression\n",
    "\n",
    "Each algorithm will be evaluated on compression ratio, processing speed, and signal quality preservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540b1605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize compression algorithms\n",
    "algorithms = {\n",
    "    'adaptive_lz': NeuralCompressor(algorithm='adaptive_lz', real_time=True),\n",
    "    'quantization_8bit': QuantizationCompressor(bits=8, adaptive=True),\n",
    "    'quantization_12bit': QuantizationCompressor(bits=12, adaptive=True),\n",
    "    'wavelet_db4': WaveletCompressor(wavelet='db4', levels=5, threshold=0.1),\n",
    "    'wavelet_db8': WaveletCompressor(wavelet='db8', levels=6, threshold=0.05),\n",
    "}\n",
    "\n",
    "# For autoencoder, we need to train it first\n",
    "autoencoder = AutoencoderCompressor(latent_dim=32, epochs=50)\n",
    "\n",
    "print(\"Compression algorithms initialized:\")\n",
    "for name, algo in algorithms.items():\n",
    "    print(f\"- {name}: {type(algo).__name__}\")\n",
    "\n",
    "print(f\"- autoencoder: {type(autoencoder).__name__} (requires training)\")\n",
    "\n",
    "# Prepare test data (use preprocessed data)\n",
    "test_data = preprocessed_data.astype(np.float32)  # Ensure consistent data type\n",
    "print(f\"\\nTest data shape: {test_data.shape}\")\n",
    "print(f\"Test data size: {test_data.nbytes / 1e6:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09aa1b9",
   "metadata": {},
   "source": [
    "## Compression Algorithm Evaluation\n",
    "\n",
    "We'll evaluate each compression algorithm on multiple metrics:\n",
    "- **Compression Ratio**: Original size / Compressed size\n",
    "- **Compression Time**: Time to compress the data\n",
    "- **Decompression Time**: Time to decompress the data\n",
    "- **Signal Quality**: SNR and MSE after reconstruction\n",
    "- **Throughput**: Data processing rate (MB/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7018278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate compression algorithm\n",
    "def evaluate_compression(algorithm, data, algorithm_name):\n",
    "    \"\"\"Evaluate compression algorithm performance.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Measure compression\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        compressed_data = algorithm.compress(data)\n",
    "        compression_time = time.time() - start_time\n",
    "        \n",
    "        # Measure decompression\n",
    "        start_time = time.time()\n",
    "        decompressed_data = algorithm.decompress(compressed_data)\n",
    "        decompression_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        original_size = data.nbytes\n",
    "        compressed_size = len(compressed_data)\n",
    "        compression_ratio = original_size / compressed_size\n",
    "        \n",
    "        # Signal quality metrics\n",
    "        if decompressed_data.shape != data.shape:\n",
    "            # Handle shape mismatch (simplified reconstruction)\n",
    "            decompressed_data = decompressed_data.reshape(data.shape)\n",
    "        \n",
    "        mse = np.mean((data - decompressed_data) ** 2)\n",
    "        signal_power = np.var(data)\n",
    "        snr_db = 10 * np.log10(signal_power / mse) if mse > 0 else float('inf')\n",
    "        \n",
    "        # Throughput\n",
    "        total_time = compression_time + decompression_time\n",
    "        throughput_mbps = (original_size / 1e6) / total_time if total_time > 0 else 0\n",
    "        \n",
    "        results = {\n",
    "            'Algorithm': algorithm_name,\n",
    "            'Compression_Ratio': compression_ratio,\n",
    "            'Compression_Time_ms': compression_time * 1000,\n",
    "            'Decompression_Time_ms': decompression_time * 1000,\n",
    "            'Total_Time_ms': total_time * 1000,\n",
    "            'Throughput_MBps': throughput_mbps,\n",
    "            'SNR_dB': snr_db,\n",
    "            'MSE': mse,\n",
    "            'Original_Size_MB': original_size / 1e6,\n",
    "            'Compressed_Size_MB': compressed_size / 1e6,\n",
    "            'Status': 'Success'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        results = {\n",
    "            'Algorithm': algorithm_name,\n",
    "            'Compression_Ratio': 0,\n",
    "            'Compression_Time_ms': 0,\n",
    "            'Decompression_Time_ms': 0,\n",
    "            'Total_Time_ms': 0,\n",
    "            'Throughput_MBps': 0,\n",
    "            'SNR_dB': 0,\n",
    "            'MSE': float('inf'),\n",
    "            'Original_Size_MB': data.nbytes / 1e6,\n",
    "            'Compressed_Size_MB': 0,\n",
    "            'Status': f'Failed: {str(e)}'\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run benchmarks\n",
    "print(\"Running compression benchmarks...\")\n",
    "benchmark_results = []\n",
    "\n",
    "for name, algorithm in algorithms.items():\n",
    "    print(f\"\\nEvaluating {name}...\")\n",
    "    result = evaluate_compression(algorithm, test_data, name)\n",
    "    benchmark_results.append(result)\n",
    "    \n",
    "    if result['Status'] == 'Success':\n",
    "        print(f\"  Compression ratio: {result['Compression_Ratio']:.2f}:1\")\n",
    "        print(f\"  Processing time: {result['Total_Time_ms']:.2f} ms\")\n",
    "        print(f\"  Throughput: {result['Throughput_MBps']:.2f} MB/s\")\n",
    "        print(f\"  SNR: {result['SNR_dB']:.2f} dB\")\n",
    "    else:\n",
    "        print(f\"  {result['Status']}\")\n",
    "\n",
    "# Train and evaluate autoencoder separately\n",
    "print(f\"\\nTraining autoencoder on subset of data...\")\n",
    "training_data = test_data[:, :10000]  # Use first 10 seconds for training\n",
    "autoencoder.fit(training_data)\n",
    "\n",
    "print(\"Evaluating autoencoder...\")\n",
    "result = evaluate_compression(autoencoder, test_data, 'autoencoder')\n",
    "benchmark_results.append(result)\n",
    "\n",
    "if result['Status'] == 'Success':\n",
    "    print(f\"  Compression ratio: {result['Compression_Ratio']:.2f}:1\")\n",
    "    print(f\"  Processing time: {result['Total_Time_ms']:.2f} ms\")\n",
    "    print(f\"  Throughput: {result['Throughput_MBps']:.2f} MB/s\")\n",
    "    print(f\"  SNR: {result['SNR_dB']:.2f} dB\")\n",
    "else:\n",
    "    print(f\"  {result['Status']}\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(benchmark_results)\n",
    "print(f\"\\nBenchmark completed! {len(benchmark_results)} algorithms evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbb719f",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization\n",
    "\n",
    "Now we'll analyze the benchmark results and create visualizations to compare the performance of different compression algorithms across various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0080a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results table\n",
    "successful_results = results_df[results_df['Status'] == 'Success'].copy()\n",
    "\n",
    "print(\"Compression Algorithm Performance Summary:\")\n",
    "print(\"=\" * 80)\n",
    "display_cols = ['Algorithm', 'Compression_Ratio', 'Total_Time_ms', 'Throughput_MBps', 'SNR_dB']\n",
    "print(successful_results[display_cols].round(2))\n",
    "\n",
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "if len(successful_results) > 0:\n",
    "    # 1. Compression Ratio Comparison\n",
    "    ax1 = axes[0, 0]\n",
    "    bars1 = ax1.bar(successful_results['Algorithm'], successful_results['Compression_Ratio'])\n",
    "    ax1.set_title('Compression Ratio Comparison')\n",
    "    ax1.set_ylabel('Compression Ratio')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.1f}:1', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. Processing Time Comparison\n",
    "    ax2 = axes[0, 1]\n",
    "    bars2 = ax2.bar(successful_results['Algorithm'], successful_results['Total_Time_ms'])\n",
    "    ax2.set_title('Total Processing Time')\n",
    "    ax2.set_ylabel('Time (ms)')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Throughput Comparison\n",
    "    ax3 = axes[0, 2]\n",
    "    bars3 = ax3.bar(successful_results['Algorithm'], successful_results['Throughput_MBps'])\n",
    "    ax3.set_title('Throughput Comparison')\n",
    "    ax3.set_ylabel('Throughput (MB/s)')\n",
    "    ax3.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 4. Signal Quality (SNR)\n",
    "    ax4 = axes[1, 0]\n",
    "    bars4 = ax4.bar(successful_results['Algorithm'], successful_results['SNR_dB'])\n",
    "    ax4.set_title('Signal Quality (SNR)')\n",
    "    ax4.set_ylabel('SNR (dB)')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 5. Compression Ratio vs SNR Trade-off\n",
    "    ax5 = axes[1, 1]\n",
    "    scatter = ax5.scatter(successful_results['Compression_Ratio'], successful_results['SNR_dB'], \n",
    "                         s=100, alpha=0.7)\n",
    "    ax5.set_xlabel('Compression Ratio')\n",
    "    ax5.set_ylabel('SNR (dB)')\n",
    "    ax5.set_title('Compression vs Quality Trade-off')\n",
    "    \n",
    "    # Add labels for each point\n",
    "    for i, row in successful_results.iterrows():\n",
    "        ax5.annotate(row['Algorithm'], \n",
    "                    (row['Compression_Ratio'], row['SNR_dB']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    \n",
    "    # 6. Efficiency Plot (Compression Ratio vs Throughput)\n",
    "    ax6 = axes[1, 2]\n",
    "    scatter2 = ax6.scatter(successful_results['Compression_Ratio'], successful_results['Throughput_MBps'],\n",
    "                          s=100, alpha=0.7, c=successful_results['SNR_dB'], cmap='viridis')\n",
    "    ax6.set_xlabel('Compression Ratio')\n",
    "    ax6.set_ylabel('Throughput (MB/s)')\n",
    "    ax6.set_title('Efficiency: Compression vs Speed')\n",
    "    \n",
    "    # Add colorbar for SNR\n",
    "    cbar = plt.colorbar(scatter2, ax=ax6)\n",
    "    cbar.set_label('SNR (dB)')\n",
    "    \n",
    "    # Add labels\n",
    "    for i, row in successful_results.iterrows():\n",
    "        ax6.annotate(row['Algorithm'], \n",
    "                    (row['Compression_Ratio'], row['Throughput_MBps']),\n",
    "                    xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "else:\n",
    "    # If no successful results, show message\n",
    "    for ax in axes.flat:\n",
    "        ax.text(0.5, 0.5, 'No successful compression results to display', \n",
    "                ha='center', va='center', transform=ax.transAxes, fontsize=12)\n",
    "        ax.set_title('No Data')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance ranking\n",
    "if len(successful_results) > 0:\n",
    "    print(\"\\nAlgorithm Rankings:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Rank by compression ratio\n",
    "    ranking_compression = successful_results.nlargest(3, 'Compression_Ratio')\n",
    "    print(\"Top 3 by Compression Ratio:\")\n",
    "    for i, (_, row) in enumerate(ranking_compression.iterrows()):\n",
    "        print(f\"  {i+1}. {row['Algorithm']}: {row['Compression_Ratio']:.2f}:1\")\n",
    "    \n",
    "    # Rank by speed\n",
    "    ranking_speed = successful_results.nlargest(3, 'Throughput_MBps')\n",
    "    print(\"\\nTop 3 by Processing Speed:\")\n",
    "    for i, (_, row) in enumerate(ranking_speed.iterrows()):\n",
    "        print(f\"  {i+1}. {row['Algorithm']}: {row['Throughput_MBps']:.2f} MB/s\")\n",
    "    \n",
    "    # Rank by signal quality\n",
    "    ranking_quality = successful_results.nlargest(3, 'SNR_dB')\n",
    "    print(\"\\nTop 3 by Signal Quality:\")\n",
    "    for i, (_, row) in enumerate(ranking_quality.iterrows()):\n",
    "        print(f\"  {i+1}. {row['Algorithm']}: {row['SNR_dB']:.2f} dB\")\n",
    "    \n",
    "    # Overall score (weighted combination)\n",
    "    successful_results['Overall_Score'] = (\n",
    "        0.4 * successful_results['Compression_Ratio'] / successful_results['Compression_Ratio'].max() +\n",
    "        0.3 * successful_results['Throughput_MBps'] / successful_results['Throughput_MBps'].max() +\n",
    "        0.3 * successful_results['SNR_dB'] / successful_results['SNR_dB'].max()\n",
    "    )\n",
    "    \n",
    "    ranking_overall = successful_results.nlargest(3, 'Overall_Score')\n",
    "    print(\"\\nTop 3 Overall (Weighted Score):\")\n",
    "    for i, (_, row) in enumerate(ranking_overall.iterrows()):\n",
    "        print(f\"  {i+1}. {row['Algorithm']}: Score {row['Overall_Score']:.3f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"\\nNo successful results to rank.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
