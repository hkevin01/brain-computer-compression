{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a4188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Check CUDA availability\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cudf\n",
    "    CUDA_AVAILABLE = cp.cuda.is_available()\n",
    "    if CUDA_AVAILABLE:\n",
    "        %load_ext cudf.pandas\n",
    "        print(\"üéâ CUDA is available! Using GPU acceleration\")\n",
    "        print(f\"CUDA Version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "        print(f\"cuDF Version: {cudf.__version__}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è CUDA is available but no GPU was detected\")\n",
    "except ImportError:\n",
    "    CUDA_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è CUDA tools not available. Installing required packages:\")\n",
    "    print(\"pip install cupy-cuda12x cudf-cuda12x\")  # Adjust version as needed\n",
    "\n",
    "# For non-GPU operations we'll still need pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Set pandas to use cuDF when possible if CUDA is available\n",
    "if CUDA_AVAILABLE:\n",
    "    # This will make pd.read_csv, pd.DataFrame etc. use GPU automatically\n",
    "    pd.set_option('compute.use_cudf', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0986ca",
   "metadata": {},
   "source": [
    "# CUDA Acceleration for BCI Data Compression\n",
    "\n",
    "This notebook demonstrates how to use CUDA acceleration via cuDF to improve the performance of our BCI data compression pipeline. We'll cover:\n",
    "\n",
    "1. Setting up the CUDA environment\n",
    "2. Initializing cuDF\n",
    "3. Converting operations to use GPU acceleration\n",
    "4. Benchmarking performance improvements\n",
    "5. Managing GPU memory effectively"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c56989a",
   "metadata": {},
   "source": [
    "## 1. Setting Up the CUDA Environment\n",
    "\n",
    "First, let's verify our CUDA environment and install necessary dependencies. We'll need:\n",
    "- CUDA Toolkit\n",
    "- cuDF\n",
    "- cuPy\n",
    "- numba\n",
    "\n",
    "We'll check the GPU availability and CUDA version first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4050cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cudf\n",
    "    from numba import cuda\n",
    "\n",
    "    # Check CUDA availability\n",
    "    print(f\"CUDA available: {cp.cuda.is_available()}\")\n",
    "    print(f\"CUDA version: {cp.cuda.runtime.runtimeGetVersion()}\")\n",
    "    print(f\"Number of GPUs: {cp.cuda.runtime.getDeviceCount()}\")\n",
    "\n",
    "    # Get GPU device information\n",
    "    device = cp.cuda.runtime.getDevice()\n",
    "    props = cp.cuda.runtime.getDeviceProperties(device)\n",
    "    print(f\"\\nGPU Device: {props['name'].decode()}\")\n",
    "    print(f\"Compute Capability: {props['major']}.{props['minor']}\")\n",
    "    print(f\"Total Memory: {props['totalGlobalMem'] / 1e9:.2f} GB\")\n",
    "\n",
    "    CUDA_AVAILABLE = True\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"CUDA environment not available: {e}\")\n",
    "    print(\"Will fall back to CPU implementation\")\n",
    "    CUDA_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73aa3f00",
   "metadata": {},
   "source": [
    "## 2. Initializing cuDF\n",
    "\n",
    "Now that we've verified our CUDA environment, let's initialize cuDF and configure it to work with our existing pandas code. We'll:\n",
    "1. Load the cuDF extension\n",
    "2. Configure pandas API compatibility\n",
    "3. Set up memory management preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aebb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA_AVAILABLE:\n",
    "    # Load cuDF extension for pandas API compatibility\n",
    "    %load_ext cudf.pandas\n",
    "\n",
    "    # Configure pandas API settings\n",
    "    import pandas as pd\n",
    "    pd.set_option('compute.use_numba', True)\n",
    "    pd.set_option('compute.use_cudf', True)\n",
    "\n",
    "    # Set up cuDF memory pool\n",
    "    import rmm\n",
    "    rmm.reinitialize(\n",
    "        pool_allocator=True,\n",
    "        initial_pool_size=1 << 30  # 1GB initial pool\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ cuDF initialized successfully\")\n",
    "    print(\"‚úÖ Pandas API compatibility enabled\")\n",
    "    print(\"‚úÖ Memory pool configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabf132b",
   "metadata": {},
   "source": [
    "## 3. Converting Operations to Use GPU Acceleration\n",
    "\n",
    "Let's convert some common BCI data operations to use GPU acceleration. We'll demonstrate with:\n",
    "1. Loading and preprocessing neural data\n",
    "2. Feature extraction\n",
    "3. Signal filtering\n",
    "4. Data compression operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18946800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data(n_channels=32, n_samples=10000):\n",
    "    \"\"\"Generate sample neural data\"\"\"\n",
    "    return np.random.randn(n_channels, n_samples)\n",
    "\n",
    "# Create sample data\n",
    "data = create_sample_data()\n",
    "print(\"Created sample data:\", data.shape)\n",
    "\n",
    "if CUDA_AVAILABLE:\n",
    "    # Convert to GPU arrays\n",
    "    gpu_data = cp.asarray(data)\n",
    "    print(\"\\nData transferred to GPU:\", gpu_data.shape)\n",
    "\n",
    "    # Example 1: Basic preprocessing\n",
    "    def preprocess_gpu(data):\n",
    "        # Z-score normalization\n",
    "        mean = cp.mean(data, axis=1, keepdims=True)\n",
    "        std = cp.std(data, axis=1, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "\n",
    "    normalized_data = preprocess_gpu(gpu_data)\n",
    "    print(\"Normalized data stats:\")\n",
    "    print(f\"Mean: {cp.mean(normalized_data):.3f}\")\n",
    "    print(f\"Std: {cp.std(normalized_data):.3f}\")\n",
    "\n",
    "    # Example 2: Feature extraction using cuDF\n",
    "    def extract_features_gpu(data):\n",
    "        df = cudf.DataFrame(data)\n",
    "        features = {\n",
    "            'mean': df.mean().values,\n",
    "            'std': df.std().values,\n",
    "            'max': df.max().values,\n",
    "            'min': df.min().values\n",
    "        }\n",
    "        return features\n",
    "\n",
    "    features = extract_features_gpu(normalized_data)\n",
    "    print(\"\\nExtracted features (first channel):\")\n",
    "    for name, value in features.items():\n",
    "        print(f\"{name}: {value[0]:.3f}\")\n",
    "else:\n",
    "    print(\"Running on CPU - GPU acceleration not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd785d",
   "metadata": {},
   "source": [
    "## 4. Benchmarking Performance\n",
    "\n",
    "Let's compare the performance between CPU and GPU implementations for some common operations:\n",
    "1. Data normalization\n",
    "2. Feature extraction\n",
    "3. Signal filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a56df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA_AVAILABLE:\n",
    "    import time\n",
    "    from scipy import signal\n",
    "\n",
    "    # Create larger dataset for meaningful benchmarks\n",
    "    large_data = create_sample_data(64, 100000)\n",
    "\n",
    "    def benchmark_operation(name, cpu_func, gpu_func, data, n_runs=5):\n",
    "        # CPU timing\n",
    "        cpu_times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            cpu_result = cpu_func(data)\n",
    "            cpu_times.append(time.time() - start)\n",
    "\n",
    "        # GPU timing\n",
    "        gpu_data = cp.asarray(data)\n",
    "        gpu_times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            gpu_result = gpu_func(gpu_data)\n",
    "            cp.cuda.Stream.null.synchronize()\n",
    "            gpu_times.append(time.time() - start)\n",
    "\n",
    "        avg_cpu = np.mean(cpu_times)\n",
    "        avg_gpu = np.mean(gpu_times)\n",
    "        speedup = avg_cpu / avg_gpu\n",
    "\n",
    "        print(f\"\\n{name} Benchmark:\")\n",
    "        print(f\"CPU time: {avg_cpu:.4f}s\")\n",
    "        print(f\"GPU time: {avg_gpu:.4f}s\")\n",
    "        print(f\"Speedup: {speedup:.1f}x\")\n",
    "\n",
    "    # 1. Normalization benchmark\n",
    "    def cpu_normalize(data):\n",
    "        mean = np.mean(data, axis=1, keepdims=True)\n",
    "        std = np.std(data, axis=1, keepdims=True)\n",
    "        return (data - mean) / std\n",
    "\n",
    "    benchmark_operation(\n",
    "        \"Normalization\",\n",
    "        cpu_normalize,\n",
    "        preprocess_gpu,\n",
    "        large_data\n",
    "    )\n",
    "\n",
    "    # 2. Feature extraction benchmark\n",
    "    def cpu_extract_features(data):\n",
    "        df = pd.DataFrame(data)\n",
    "        return {\n",
    "            'mean': df.mean().values,\n",
    "            'std': df.std().values,\n",
    "            'max': df.max().values,\n",
    "            'min': df.min().values\n",
    "        }\n",
    "\n",
    "    benchmark_operation(\n",
    "        \"Feature Extraction\",\n",
    "        cpu_extract_features,\n",
    "        extract_features_gpu,\n",
    "        large_data\n",
    "    )\n",
    "\n",
    "else:\n",
    "    print(\"GPU benchmarking not available - CUDA required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48f06e",
   "metadata": {},
   "source": [
    "## 5. Memory Management\n",
    "\n",
    "Proper GPU memory management is crucial for optimal performance. Here's how to:\n",
    "1. Monitor memory usage\n",
    "2. Clear unused memory\n",
    "3. Handle large datasets efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ecc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA_AVAILABLE:\n",
    "    def print_memory_stats():\n",
    "        \"\"\"Print current GPU memory usage\"\"\"\n",
    "        free, total = cp.cuda.runtime.memGetInfo()\n",
    "        used = total - free\n",
    "        print(f\"GPU Memory Usage:\")\n",
    "        print(f\"Used: {used / 1e9:.2f} GB\")\n",
    "        print(f\"Free: {free / 1e9:.2f} GB\")\n",
    "        print(f\"Total: {total / 1e9:.2f} GB\")\n",
    "\n",
    "    # Initial memory state\n",
    "    print(\"Initial memory state:\")\n",
    "    print_memory_stats()\n",
    "\n",
    "    # Create some large arrays\n",
    "    print(\"\\nAllocating large arrays...\")\n",
    "    arrays = []\n",
    "    for i in range(3):\n",
    "        arrays.append(cp.random.randn(10000, 10000))\n",
    "        print(f\"\\nAfter allocating array {i+1}:\")\n",
    "        print_memory_stats()\n",
    "\n",
    "    # Clear memory\n",
    "    print(\"\\nClearing memory...\")\n",
    "    arrays.clear()\n",
    "    cp.get_default_memory_pool().free_all_blocks()\n",
    "    print_memory_stats()\n",
    "\n",
    "    # Demonstrate proper context management\n",
    "    print(\"\\nUsing context manager for automatic cleanup...\")\n",
    "    with cp.cuda.Device(0):\n",
    "        # Work with temporary arrays\n",
    "        temp_array = cp.random.randn(5000, 5000)\n",
    "        print(\"\\nInside context:\")\n",
    "        print_memory_stats()\n",
    "\n",
    "    print(\"\\nAfter context exit:\")\n",
    "    print_memory_stats()\n",
    "\n",
    "else:\n",
    "    print(\"Memory management demo not available - CUDA required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e295bfe0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've demonstrated how to:\n",
    "1. Set up CUDA acceleration with cuDF\n",
    "2. Convert existing operations to use GPU processing\n",
    "3. Achieve significant speedups for common operations\n",
    "4. Properly manage GPU memory\n",
    "\n",
    "For best results:\n",
    "- Monitor memory usage carefully\n",
    "- Use context managers for automatic cleanup\n",
    "- Batch operations when possible\n",
    "- Profile performance to identify bottlenecks\n",
    "\n",
    "Remember to check `CUDA_AVAILABLE` before using GPU operations and provide CPU fallbacks for compatibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1fa0a37",
   "metadata": {},
   "source": [
    "## 6. Scikit-learn Acceleration with cuML\n",
    "\n",
    "We can accelerate scikit-learn operations using RAPIDS cuML, which provides GPU-accelerated versions of common machine learning algorithms. This is particularly useful for:\n",
    "- Dimensionality reduction (PCA, UMAP)\n",
    "- Clustering (K-means, DBSCAN)\n",
    "- Classification and regression\n",
    "- Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865dec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA_AVAILABLE:\n",
    "    try:\n",
    "        # Load cuML extension for scikit-learn acceleration\n",
    "        %load_ext cuml.accel\n",
    "\n",
    "        import cuml\n",
    "        from cuml.preprocessing import StandardScaler\n",
    "        from cuml.decomposition import PCA\n",
    "        from cuml.cluster import KMeans\n",
    "        print(\"‚úÖ cuML successfully loaded\")\n",
    "\n",
    "        # Generate sample data for ML tasks\n",
    "        n_samples = 10000\n",
    "        n_features = 32\n",
    "        X = cp.random.randn(n_samples, n_features)\n",
    "\n",
    "        # Example 1: Standardization and PCA\n",
    "        print(\"\\nüîÑ Testing preprocessing and dimensionality reduction...\")\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "        pca = PCA(n_components=8)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "        print(f\"Original shape: {X.shape}\")\n",
    "        print(f\"After PCA: {X_pca.shape}\")\n",
    "        print(f\"Explained variance ratio: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "        # Example 2: Clustering\n",
    "        print(\"\\nüîç Testing clustering...\")\n",
    "        kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "        clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "        print(f\"Number of samples in each cluster:\")\n",
    "        unique, counts = cp.unique(clusters, return_counts=True)\n",
    "        for cluster, count in zip(unique, counts):\n",
    "            print(f\"Cluster {cluster}: {count} samples\")\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"cuML not available: {e}\")\n",
    "        print(\"Install cuML with: pip install cuml\")\n",
    "else:\n",
    "    print(\"CUDA required for cuML acceleration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb899aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA_AVAILABLE:\n",
    "    try:\n",
    "        import sklearn\n",
    "        from sklearn.preprocessing import StandardScaler as CPUStandardScaler\n",
    "        from sklearn.decomposition import PCA as CPUPca\n",
    "        from sklearn.cluster import KMeans as CPUKMeans\n",
    "\n",
    "        print(\"üî• Benchmarking scikit-learn vs cuML...\")\n",
    "\n",
    "        # Create larger dataset for meaningful comparison\n",
    "        n_samples = 50000\n",
    "        n_features = 64\n",
    "        X_cpu = np.random.randn(n_samples, n_features)\n",
    "        X_gpu = cp.asarray(X_cpu)\n",
    "\n",
    "        def benchmark_ml(name, cpu_func, gpu_func, cpu_data, gpu_data):\n",
    "            # CPU timing\n",
    "            cpu_start = time.time()\n",
    "            cpu_result = cpu_func(cpu_data)\n",
    "            cpu_time = time.time() - cpu_start\n",
    "\n",
    "            # GPU timing\n",
    "            gpu_start = time.time()\n",
    "            gpu_result = gpu_func(gpu_data)\n",
    "            cp.cuda.Stream.null.synchronize()\n",
    "            gpu_time = time.time() - gpu_start\n",
    "\n",
    "            speedup = cpu_time / gpu_time\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"CPU time: {cpu_time:.3f}s\")\n",
    "            print(f\"GPU time: {gpu_time:.3f}s\")\n",
    "            print(f\"Speedup: {speedup:.1f}x\")\n",
    "            return cpu_result, gpu_result\n",
    "\n",
    "        # Test 1: StandardScaler\n",
    "        print(\"\\nüìä Testing StandardScaler...\")\n",
    "        cpu_scaler = CPUStandardScaler()\n",
    "        gpu_scaler = StandardScaler()\n",
    "\n",
    "        benchmark_ml(\n",
    "            \"StandardScaler\",\n",
    "            lambda x: cpu_scaler.fit_transform(x),\n",
    "            lambda x: gpu_scaler.fit_transform(x),\n",
    "            X_cpu, X_gpu\n",
    "        )\n",
    "\n",
    "        # Test 2: PCA\n",
    "        print(\"\\nüîÑ Testing PCA...\")\n",
    "        cpu_pca = CPUPca(n_components=16)\n",
    "        gpu_pca = PCA(n_components=16)\n",
    "\n",
    "        benchmark_ml(\n",
    "            \"PCA\",\n",
    "            lambda x: cpu_pca.fit_transform(x),\n",
    "            lambda x: gpu_pca.fit_transform(x),\n",
    "            X_cpu, X_gpu\n",
    "        )\n",
    "\n",
    "        # Test 3: KMeans\n",
    "        print(\"\\nüîç Testing KMeans...\")\n",
    "        cpu_kmeans = CPUKMeans(n_clusters=8, random_state=42)\n",
    "        gpu_kmeans = KMeans(n_clusters=8, random_state=42)\n",
    "\n",
    "        benchmark_ml(\n",
    "            \"KMeans\",\n",
    "            lambda x: cpu_kmeans.fit_predict(x),\n",
    "            lambda x: gpu_kmeans.fit_predict(x),\n",
    "            X_cpu, X_gpu\n",
    "        )\n",
    "\n",
    "    except ImportError as e:\n",
    "        print(f\"Benchmarking skipped - required packages not available: {e}\")\n",
    "else:\n",
    "    print(\"GPU benchmarking not available - CUDA required\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
